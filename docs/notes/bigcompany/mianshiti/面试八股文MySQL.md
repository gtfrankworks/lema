---
title: 面试八股文MySQL
createTime: 2025/08/27 16:03:12
permalink: /bigcompany/tqi3ts1g/
---
# MySQL专栏面试题

## 1.Mysql的索引使用有哪些注意事项

### 1.1.索引哪些情况会失效

> InnoDB引擎里面有两种索引类型，一种是主键索引、一种是普通索引。 InnoDB用了B+树的结构来存储索引数据。 当使用索引列进行数据查询的时候，最终会到主键索引树中查询对应的数据行进行返回。

1. 在索引列上进行运算或使用函数会导致 MySQL 无法识别索引列，从而无法使用索引。MySQL 8 引入了函数索引来解决这个问题。
2. 在组合索引中，需要按照最左匹配的原则进行顺序检索，否则不会走索引。索引的存储结构也需要与查询中的顺序匹配，否则 InnoDB 无法识别索引导致失效。
3. 当索引列存在隐式转化时，例如索引列是字符串类型但在查询中未使用引号，MySQL 会自动进行类型转化，导致索引失效。
4. 在索引列使用不等于号（<>）或 NOT 查询的时候，由于索引数据的检索效率较低，MySQL 引擎会判断不走索引。
5. 使用通配符%xxx进行后缀匹配时，不符合最左匹配原则，所以不会走索引。然而，如果通配符匹配的是前缀xxx%，符合最左匹配，就会应用索引。
6. 当使用 OR 连接查询时，如果 OR 语句前后没有同时使用索引，索引会失效。只有 OR 左右查询字段都是索引列时，才会生效。
7. 对于多表连接查询的场景，连接顺序也会影响索引的使用。
8. 可以使用 EXPLAIN 命令来查看 SQL 的执行计划，根据执行计划进行调优。这是一个很好的方法来确定是否使用了索引以及如何改进查询性能。

### 1.2.索引不适合哪些场景

1. `数据量少的情况`：当数据量较小并且可以在很短的时间内完全加载到内存中时，使用索引可能会增加额外的开销，并且不会显著提高查询性能。在这种情况下，可以考虑直接遍历或简单查询来满足需求。
2. `高度频繁的数据更新场景`：如果数据表需要频繁地进行插入、更新或删除操作，索引会增加维护开销。每次数据更新都需要更新索引，这可能会导致性能下降。在这种情况下，需要权衡索引带来的性能提升和维护开销之间的平衡。
3. `区分度低的字段`：索引的主要目的是通过快速定位到具有特定值的数据行，提高查询效率。然而，对于区分度低的字段（例如只有少数几个不同的取值），使用索引可能没有明显的性能提升，甚至可能会增加查询成本。在这种情况下，可以考虑避免为这些字段创建索引。

### 1.3.索引的一些潜规则

1. `覆盖索引`：当查询只需要访问索引中的数据列时，可以通过创建覆盖索引来避免回表操作。覆盖索引包含了所有需要查询的数据列，这样查询可以直接从索引中获取所需数据，而无需额外的IO操作。
2. `回表`：当查询需要访问非索引列时，在使用普通索引进行查询后，数据库需要通过回表操作访问数据行以获取非索引列的值。回表操作增加了额外的IO开销，因此需要权衡索引的创建与IO代价之间的平衡。
3. `索引数据结构（B+树）`：大多数关系型数据库使用B+树作为索引的数据结构。B+树具有平衡的高度和顺序访问特性，可以快速定位到所需数据，适用于范围查询和排序操作。
4. `最左前缀原则`：在多列组合索引中，最左前缀原则指出，如果查询条件只使用了组合索引的前缀列，那么数据库仍然可以使用该索引。例如，如果索引是（A, B, C），而查询只使用了条件 A = x，则索引仍然有效。
5. `索引下推`：索引下推是一种优化技术，可以在数据库引擎内部将查询条件下推至存储引擎来减少数据行的读取。通过索引下推，可以在索引层过滤掉不符合查询条件的数据行，减少IO开销。

## 2.MySQL遇到过死锁问题吗，你是如何解决的？

1. `检查死锁信息`：使用`SHOW ENGINE INNODB STATUS`命令或查询`information_schema.INNODB_TRX`和`information_schema.INNODB_LOCKS`系统表来获取死锁信息。这将提供有关涉及的事务、资源和死锁链的详细信息。
2. `重试事务`：当检测到死锁时，可以捕获该异常并在稍后重试事务。通过延迟一段时间后重新尝试，可以希望死锁情况得到解决。
3. `设定超时等待`：在MySQL中，可以通过设置`innodb_lock_wait_timeout`参数来指定锁等待的超时时间。超过该时间后，等待锁的事务将被自动回滚，以避免长时间的死锁阻塞。
4. `调整事务隔离级别`：将事务隔离级别调整为较低级别（如READ COMMITTED）可以减少死锁的可能性。但请注意，较低的隔离级别可能会导致不可重复读和幻读的问题。
5. `优化查询语句和索引`：通过合理设计表结构、查询语句和索引，可以减少查询的锁定时间和冲突。确保查询只锁定必要的行，并尽量缩小事务范围。
6. `以相同的顺序访问索引记录和表`：如果多个事务同时访问相同的数据，确保它们以相同的顺序获取锁。这样可以减少死锁的概率。
7. `一次锁定所需资源`：对于需要同时操作多个资源的事务，尽量在事务开始时一次性锁定所需的所有资源。例如，使用MyISAM引擎可进行表级锁定。
8. `避免大事务`：将大事务拆分为多个小事务，减少事务持有锁的时间。这样可以降低死锁的风险，并提高并发性能。
9. `避免并发读写冲突`：尽量避免多个并发事务同时对同一表进行读写操作，特别是执行加锁且操作数据量较大的语句。可以通过合理的业务设计来减少这种情况的发生。
10. `重新设计应用逻辑`：如果死锁问题频繁发生，可能需要重新审视应用逻辑和事务设计。合理规避不必要的冲突，避免事务之间过度依赖和冲突。

------

**补充**：当使用`SHOW ENGINE INNODB STATUS`命令检测到死锁时，它会在输出结果中显示有关死锁的详细信息。以下是示例输出结果的一部分，其中涉及到死锁信息：

```mysql
------------
TRANSACTIONS
------------
Trx id counter 12345678
Purge done for trx's n:o <12345678 undo n:o <0 state: running but idle
History list length 1000
LIST OF TRANSACTIONS FOR EACH SESSION:
---TRANSACTION 123456789, ACTIVE 10 sec
2 lock struct(s), heap size 360, 1 row lock(s), undo log entries 1
MySQL thread id 12345, OS thread handle 0xabcde, query id 67890 localhost user@example.com
Trx read view will not see trx with id >= 123456789, sees < 123456789
LOCK WAIT 3 lock(s), active 2 second(s)
MySQL thread id 78901, OS thread handle 0xfedcba, query id 234567890 localhost user@example.com updating
DELETE FROM table WHERE id = 123
------- TRX HAS BEEN WAITING 2 SEC FOR THIS LOCK TO BE GRANTED:
RECORD LOCKS space id 12345 page no 6 n bits 80 index PRIMARY of table `database`.`table` trx id 123456789 lock_mode X waiting
Record lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; info bits 0
```

上述示例输出中的关键部分包含以下信息：

- `TRANSACTIONS`：事务相关信息的部分，包括活动事务的数量、事务ID计数器和历史列表长度。
- `LIST OF TRANSACTIONS FOR EACH SESSION`：列出每个会话的事务信息，包括事务ID、活动状态和持有的锁资源数量。
- `LOCK WAIT`：指示正在等待锁定资源的事务，这是死锁发生的关键信息。其中，`lock(s)`显示正在等待的锁数量，`active`显示等待时间。
- `RECORD LOCKS`：显示了与死锁相关的记录锁信息，包括空间ID、页号、索引、事务ID和锁模式。

------

`INFORMATION_SCHEMA.INNODB_TRX`是一个系统表，用于查看当前活动的InnoDB事务信息。执行该查询可以获取有关正在运行的事务的详细信息，包括事务ID、事务状态、锁等待时间等。

字段包括：

- `trx_id`：事务ID。
- `trx_state`：事务状态，例如RUNNING、LOCK WAIT等。
- `trx_started`：事务开始时间。
- `trx_requested_lock_id`：正在请求的锁ID。
- `trx_wait_started`：等待开始时间（如果有）。
- `trx_weight`：事务权重。
- `trx_mysql_thread_id`：MySQL线程ID。
- `trx_query`：事务执行的查询语句。
- `trx_operation_state`：事务操作状态。
- `trx_tables_in_use`：事务使用的表数量。
- `trx_rows_locked`：事务锁定的行数。
- `trx_rows_modified`：事务修改的行数。

------

`information_schema.INNODB_LOCKS`是一个系统表，用于查看当前活动的InnoDB锁信息。执行该查询可以获取有关正在使用的锁资源的详细信息，包括锁模式、锁定的对象和持有锁的事务等。

字段包括：

- `lock_id`：锁的ID。
- `lock_trx_id`：持有该锁的事务ID。
- `lock_mode`：锁的模式，如X（独占锁）、S（共享锁）等。
- `lock_type`：锁的类型，如RECORD（记录锁）、TABLE（表级锁）等。
- `lock_table`：锁定的表名。
- `lock_index`：锁定的索引名。
- `lock_space`：锁定对象所属的表空间ID。
- `lock_page`：锁定对象所在的页号。
- `lock_rec`：锁定的记录号。

## 3.日常工作中你是怎么优化SQL的

1. 硬件及操作系统层面优化：

- `CPU`：选择高性能的CPU，适当进行CPU绑定，避免CPU资源竞争。
- `内存`：增加可用内存大小，提升缓存效果，减少磁盘IO。
- `磁盘读写速度`：使用高性能的磁盘设备，合理划分数据和日志的存储路径。
- `网络带宽`：保证足够的网络带宽，避免网络成为瓶颈。
- `文件句柄数`：增加文件句柄数，可提高并发连接数。
- `操作系统网络配置`：优化操作系统的网络参数，提升网络性能。

2. 架构设计层面优化：

- `搭建主从集群`：确保高可用性和负载均衡。
- `读写分离设计`：将读请求分发到从库，减轻主库的负载压力。
- `分库分表机制`：通过水平拆分数据，减少单个节点的读写压力。
- `引入分布式数据库`：针对热点数据引入高效的分布式数据库(mongodb,redis等)，提升性能和扩展性。

3. MySQL程序配置优化：

- `修改my.cnf配置文件`：根据硬件环境和使用场景，调整MySQL的参数配置，如最大连接数、缓存池大小等。
- `注意配置作用域和热加载`：了解配置参数的作用域和是否支持热加载，避免配置失效或重启服务导致配置丢失。

4. SQL优化：

- `慢SQL的定位和排查`：通过慢查询日志和工具分析找出执行时间较长的SQL语句。
- `执行计划分析`：使用EXPLAIN关键字查看SQL的执行计划，确定性能瓶颈并进行优化。
- `使用show profile工具`：分析SQL的资源消耗情况，包括IO开销、CPU开销和内存开销等，以进一步优化SQL的性能。

## 4.说说分库与分表的设计

> 分库分表是数据库设计中常见的优化手段，主要目的是为了解决单表数据量过大、读写性能瓶颈等问题。

1. 分库分表方案：

- `水平分库`：以字段为依据，按照一定策略(hash、range等),将一个库中的数据拆分到多个库中。每个库的结构都一样；每个库的数据都不一样，没有交集；所有库的并集是全量数据。

- `水平分表`：以字段为依据，按照一定策略(hash、range等),将一个表中的数据拆分到多个表中。

- `垂直分库`：以表为依据，按照业务归属不同，将不同的表拆分到不同的库中。

- `垂直分表`：以字段为依据，按照字段的活跃性，将表中字段拆到不同的表(主表和扩展表)中。

2. 分库分表中间件：

- `Sharding-JDBC（当当）`：Sharding-JDBC是当当网开源的一款轻量级Java分布式数据库中间件，提供了基于Java JDBC标准的数据分片功能，可以将数据按照规则分散存储到多个数据库中。
- `Mycat`：Mycat是一个开源的MySQL数据库中间件，它提供了丰富的分片策略和路由规则，并且支持水平拆分、读写分离、分布式事务等功能。
- `TDDL（淘宝）`：TDDL是淘宝开源的一个分布式数据库中间件，它内建了分库分表、读写分离、分布式事务等特性，可以将数据分散到多个数据库中。
- `Oceanus（58 同城数据库中间件）`：Oceanus是58同城开源的数据库中间件，它支持自动化的数据分片和数据迁移，可以实现高效的水平扩展和负载均衡。
- `Vitess（谷歌开发的数据库中间件）`：Vitess是由谷歌开源的数据库中间件，旨在解决大规模MySQL集群的扩展问题。它提供了分片、水平拆分、负载均衡等功能，并且可以无缝集成到现有的应用中。
- `Atlas（Qihoo 360）`：Atlas是奇虎360开源的数据库中间件，它支持分库分表和读写分离，同时提供了高可用性和容错能力。

3. 分库分表可能遇到的问题

- `事务问题`：在分库分表环境中，由于事务需要跨多个数据库节点，可以引入分布式事务管理框架，如Seata、XA等，来确保事务的一致性和隔离性。
- `跨节点Join的问题`：在分库分表环境中，跨节点Join涉及到的数据在不同的数据库节点上，可以通过分两次查询并在应用程序中进行连接操作来实现。首先在两个节点上分别查询所需的数据，然后在应用程序中进行连接操作。
- `跨节点的count、order by、group by以及聚合函数问题`：这些操作需要对数据进行聚合和排序，可以分别在各个节点上执行，并将结果传回应用程序端进行合并。应用程序可以在接收到各个节点的结果后，在内存中进行合并、排序和计算。
- `数据迁移、容量规划和扩容问题`：在进行数据迁移之前，需要仔细规划和评估数据量，合理划分分片和数据库节点。可以选择合适的工具和策略进行数据的迁移，同时需要进行充分的测试和监控来确保数据的完整性和系统的稳定性。如果需要扩容，可以动态添加新的节点，并进行数据分片的重新平衡。
- `ID问题`：在分库分表环境中，数据库的主键生成机制无法满足需求。可以考虑使用全局唯一标识符（UUID）或者其他分布式ID生成算法来生成唯一ID。这样可以避免在分片环境下产生重复的主键。
- `跨分片的排序分页问题`：在分库分表环境中，跨分片的排序分页比较复杂。可以通过增大pageSize来加大每次查询的数据量，然后在应用程序中进行排序和分页操作。也可以引入缓存技术或者使用搜索引擎等工具来加快分页查询的速度。

4. 三种应用场景

- `只分库不分表`： 当数据库的读写访问量过高，单个数据库无法满足高并发的需求时，可以考虑使用只分库的方案。通过增加数据库实例，将数据按照某种规则（如用户ID、地理位置等）划分到不同的数据库中，从而提升系统的并发性能。
- `只分表不分库`： 当单表存储的数据量非常大，导致数据库的读写性能出现瓶颈时，可以考虑使用只分表的方案。通过将原本的大表拆分成多个小表，每个表存储一部分数据，有效减少单表的数据量，提升数据的读写效率。
- `既分库又分表`： 当同时满足分库和分表的条件时，即数据库连接不够用且单表的数据量非常大，这时可以采用既分库又分表的方案。通过将数据根据某种规则划分到不同的数据库实例和表中，将负载均匀地分散到多个节点上，既提升了系统的并发性能，又减少了单表的数据量，进一步提升了读写效率。

## 5.InnoDB与MyISAM的区别

1. 主要区别：

- `数据存储方式`：MyISAM将数据和索引分开存储，而InnoDB将它们存储在同一个文件中。
- `事务支持`：MyISAM不支持事务，而InnoDB支持ACID特性的事务处理。
- `锁的支持`：MyISAM只支持表级锁定，而InnoDB支持行级锁定、表级锁定、间隙锁定和临键锁定等多种类型的锁定方式。
- `外键支持`：MyISAM不支持外键约束，而InnoDB支持外键约束。

2. 实际开发中如何选择:

- `是否需要事务支持`：如果需要支持事务处理，确保数据的一致性和完整性，选择InnoDB。InnoDB支持ACID特性的事务。
- `读写操作情况`：如果大部分表操作都是查询，且对事务支持和数据完整性要求不高，则选择MyISAM。如果有频繁的写入操作，并且需要事务支持和更好的并发性能，则选择InnoDB。
- `数据完整性要求`：如果对数据完整性有较高的要求，需要使用外键约束来保持数据一致性，选择InnoDB。InnoDB支持外键约束，并且可以自动处理级联删除和更新等操作。
- `并发写入需求`：如果有大量并发的写入操作，并且可能会有锁冲突的情况，选择InnoDB以减少锁竞争和提高并发写入性能。MyISAM的表级锁定可能导致写入操作的堵塞。
- `备份与恢复`：如果对备份和恢复速度有较高要求，可以选择MyISAM。MyISAM的备份和恢复速度较快。而InnoDB由于支持事务和行级锁定，备份和恢复过程可能更复杂和耗时。

## 6.数据库索引的原理，为什么要用 B+树，为什么不用二 叉树？

数据库索引的原理是通过使用特定的数据结构来提高数据检索的效率。B+树是一种常用的索引存储结构，而不使用二叉树的主要原因有以下几点。

1. `多路平衡树`：B+树是一种多路平衡树，相对于二叉树而言，它能存储更多的索引节点和数据节点。这意味着在相同高度的情况下，B+树可以存储更多的数据，减少了磁盘IO次数。
2. `磁盘IO优化`：数据库中的数据通常存储在磁盘上，而磁盘IO的速度较慢。B+树的设计可以最小化磁盘IO的次数。因为它将所有的数据存储在叶子节点上，并且叶子节点之间使用链表进行连接。这样的设计使得范围查询更加高效，只需要遍历有限个叶子节点即可获取所需的数据。
3. `排序和范围查询`：在数据库中，排序和范围查询是非常常见的操作。B+树通过将所有数据存储在叶子节点上，并采用有序的方式连接叶子节点，使得排序操作更加高效。而对于范围查询，B+树只需要遍历叶子节点即可获取相关数据，减少了不必要的遍历。
4. `全局扫描`：B+树的叶子节点使用链表进行连接，因此对于全局扫描操作，只需要遍历所有的叶子节点即可。而二叉树需要遍历整个树结构。
5. `主键的顺序性`：在实际应用中，很多数据库的主键都是具有顺序性的。而B+树的非叶子节点仅存储索引，可通过主键的顺序性来优化索引节点的存储和查询，提高检索效率。

综上所述，B+树相比于二叉树在磁盘IO优化、范围查询、排序、全局扫描等方面具有明显的优势，适合作为数据库索引的存储结构。

## 7.聚集索引与非聚集索引的区别

1. `结构`：聚集索引是基于主键创建的索引，它包含表中的所有数据行。而非聚集索引则是基于其他列创建的索引，只包含部分数据行。在InnoDB存储引擎中，每个表只有一个聚集索引。

2. `叶子节点`：聚集索引的叶子节点存储了表中的所有数据行，而非聚集索引的叶子节点只存储了与该非聚集索引相关的数据行。

3. `查询性能`：由于聚集索引包含了表中的所有数据行，因此在进行查询时，可以直接通过聚集索引获取到所需的数据。而非聚集索引需要先通过非聚集索引查找到相关数据行，然后再通过聚集索引获取完整的记录。因此，对于复杂的查询操作，非聚集索引可能会导致性能下降。

4. `维护成本`：由于每个表只能有一个聚集索引，因此在进行表结构变更(如添加、删除或修改列)时，需要对整个表的数据进行重新组织，这会导致大量的磁盘I/O操作和较长的执行时间。而非聚集索引则不会受到这种影响，因为它们只包含部分数据行。

5. `B+树结构`：在InnoDB存储引擎中，所有的数据行都按照主键的顺序存储在一个B+树结构中。聚集索引就是这个B+树的主节点，而非聚集索引则是B+树的其他节点。通过访问主键索引，可以快速定位到所需的数据行。

## 8.limit 1000000,10 加载很慢的话，你是怎么解决的呢？

1. 如果id是趋势递增的，那么每次查询都可以返回这次查询最大的ID,然后下次查询，加上大于上次最大id的条件，这样会通过主键索引去扫描，并且扫描数量会少很多很多。因为只需要扫描where条件的数据。这种方案适用于数据量较小的情况，如果数据量很大，则可能会导致性能下降。

   ```mysql
   select * from user WHERE id>1000000 LIMIT 10;
   ```

2. 先limit出来主键ID,然后用主表跟查询出来的ID进行inner join内连接，这样也能一定上提速，因为减少了回表，查询ID只需要走聚集索引就行。这种方案适用于数据量较大且需要频繁进行分页查询的情况，可以有效提高查询效率。

   ```mysql
   select t1.id,t1.name from `user` t1 join (select id from `user` order by id asc limit 1000000,10) t2 on
   t1.id=t2.id;
   ```

3. 当然，如果mysql级别优化不了了，我们也可以对分页数据进行缓存，比如Redis缓存，数据进行变动的时候，做好缓存依赖即可。这种方案适用于数据量较大且需要频繁进行分页查询的情况，可以有效提高查询效率。但是需要注意的是，缓存也需要占用一定的系统资源，需要根据实际情况进行权衡。

4. 因为越往后，一般用户行为触及不到，比如你去看淘宝，不会去翻后面几百页的数据，所以，业务层面也可以做一些让步，比如不做后面几百页的数据。这种方案适用于数据量较大且用户行为较为均匀分布的情况，可以有效减轻数据库的压力。

## 9.如何选择合适的分布式主键方案呢？

1. 选择合适的分布式主键方案需要考虑以下几个因素：

- `数据分布均匀性`：分布式主键要能够让数据在不同节点之间均匀分布，避免数据倾斜问题。
- `唯一性`：分布式主键要保证每个节点生成的主键都是唯一的，避免主键冲突问题。
- `可排序性`：分布式主键要能够支持按照时间顺序进行排序，方便数据的查询和分析。
- `易于生成和管理`：分布式主键的生成和管理应该是简单可靠的，不会给系统带来额外的负担。
- `性能和可扩展性`：分布式主键的生成和查询应该具备高性能和可扩展性，能够满足系统的高并发和大规模数据存储需求。

2. 基于以上因素，可以选择以下几种分布式主键方案：

- `数据库自增长序列或字段`：可以利用数据库自带的自增长序列或字段作为分布式主键，这种方式简单可靠，但是可能会导致ID重复问题。
- `UUID`:全局唯一标识符，可以保证在分布式环境下生成唯一且有序的主键。但是UUID长度较长，不适合作为表的主键。
- `Redis生成ID`:Redis自带了自增ID功能，可以作为分布式环境下的主键。但是如果使用过期时间来控制ID的生命周期，则可能会导致ID重复问题。
- `Twitter的snowflake算法`：由Twitter开源的分布式ID生成算法，可以保证在分布式环境下生成唯一且有序的主键。但是该算法对机器时钟的要求较高。
- `利用zookeeper生成唯一ID`:ZooKeeper是一个分布式协调服务，可以利用其提供的接口生成唯一ID。但是该方案需要引入额外的服务，增加了系统的复杂度。
- `MongoDB的ObjectId`:MongoDB自带的ObjectId可以作为文档的_id字段，也可以作为其他字段作为分布式环境下的主键。但是ObjectId长度较长，不适合作为表的主键。

## 10.事务的隔离级别有哪些？MySQL 的默认隔离级别是什么?

1. 什么是脏读、幻读、不可重复读:

- `脏读（Dirty Read）`：脏读是指一个事务读取了另一个事务未提交的数据。换句话说，当一个事务读取到了其他事务正在修改的数据时，尽管这些修改可能被回滚，但读取到的数据却是无效的。脏读可能导致不一致的结果。

- `幻读（Phantom Read）`：幻读是指在一个事务中多次执行相同的查询，但在这些查询之间有新的数据行出现或消失。换句话说，当一个事务在查询过程中发现了新插入或删除的数据行，就好像出现了幻觉一样。幻读可能导致查询结果的不一致性。

- `不可重复读（Non-repeatable Read）`：不可重复读是指一个事务内多次读取同一个数据，在这个过程中发现数据被其他事务修改。换句话说，当一个事务在多次读取同一数据时，发现数据的值已经发生变化，导致两次读取的结果不一致。

可以用一个例子来解释这些概念：假设有两个事务A和B。事务A首先读取了某个数据的值，然后事务B修改了该数据并提交事务。如果事务A再次读取该数据，并发现其值已经改变，这就是一个不可重复读。如果事务A在事务B提交之前读取了事务B修改后的数据，那么就出现了脏读。而如果事务A多次执行相同的查询，但在这些查询之间，事务B插入了新的符合条件的数据，并且这些新数据被查询结果包含，就发生了幻读。

2. 事务的隔离级别:

- `读未提交（Read Uncommitted）`：最低的隔离级别，允许一个事务读取另一个事务尚未提交的数据。可能导致脏读（Dirty Read）、不可重复读（Non-repeatable Read）和幻读（Phantom Read）问题。
- `读已提交（Read Committed）`：保证一个事务只能读取另一个事务已经提交的数据。可以避免脏读，但仍可能出现不可重复读和幻读问题。
- `可重复读（Repeatable Read）`：在同一事务内，重复读取相同数据会得到一致的结果。保证了可重复读，但仍可能出现幻读问题。
- `序列化（Serializable）`：最高的隔离级别，禁止并发事务对同一数据进行修改。确保了完全的隔离性，避免了所有并发问题，但可能影响系统的并发性能。

MySQL的默认隔离级别是可重复读（Repeatable Read），这是为了避免不可重复读取的问题。可以通过设置`tx_isolation`参数来更改默认隔离级别。请注意，将隔离级别设置为更高的级别可能会增加锁竞争，降低系统的并发性能。

您可以使用以下命令查看当前MySQL的隔离级别：

```mysql
SELECT @@tx_isolation;
```

## 11.在高并发情况下，如何做到安全的修改同一行数据？

1. `乐观锁`：使用乐观锁机制可以在不加锁的情况下实现并发修改同一行数据。通过增加一个版本号或时间戳字段，在提交修改时检查该字段是否与修改前一致。如果一致，则说明没有其他事务对该行数据进行修改，可以成功提交；如果不一致，则说明有其他事务已经修改了该行数据，需要进行冲突处理。
2. `悲观锁`：使用悲观锁机制可以在修改数据前先锁定该行数据，确保其他事务不能同时进行修改。这可以通过数据库提供的行级锁或者手动加锁实现。但是悲观锁会降低并发性能，因为其他事务需要等待锁的释放才能进行修改。
3. `分布式锁`：在分布式系统中，可以引入分布式锁来保证并发修改同一行数据的安全性。分布式锁可以基于共享的存储或者专门的锁服务实现，确保在同一时间只有一个事务可以修改该行数据。
4. `队列处理`：可以将并发修改请求转化为顺序执行的操作，通过将请求放入队列中逐个处理，保证每次只有一个事务修改同一行数据。这样可以避免并发冲突，但可能会降低响应时间。

## 12.数据库的乐观锁和悲观锁

1. 悲观锁（Pessimistic Locking）： 

> 悲观锁是在并发环境下，假设会有其他用户或线程对数据进行修改，因此在访问数据之前会先获取锁，阻塞其他用户或线程对数据的修改操作。悲观锁主要通过数据库的锁机制实现

可以分为以下几种类型：

- `行级锁`：锁定数据库中的某一行数据，使得其他用户或线程无法修改该行数据。例如，在MySQL中使用`SELECT ... FOR UPDATE`语句可以获取行级锁。
- `表级锁`：锁定整个表，避免其他用户或线程对表中的任何数据进行修改。例如，在MySQL中使用`LOCK TABLES`语句可以锁定整个表。
- `页级锁`：锁定数据库中的一个页，包含多行数据，可以有效减少锁的粒度。例如，InnoDB存储引擎使用页级锁。

2. 乐观锁（Optimistic Locking）： 

> 乐观锁是在并发环境下，假设并发冲突的概率较低，因此不使用显式锁进行资源保护。相反，它使用版本号（Versioning）或时间戳（Timestamp）等机制来检测并发冲突。当要更新数据时，需要先读取数据的版本号或时间戳，然后比较当前版本号与之前读取的版本号是否一致，如果一致则可以执行更新操作，否则表示数据已被修改，需要进行相应的处理。

乐观锁主要有以下几种实现方式：

- `版本号机制`：在数据库表中添加一个版本号字段，每次更新数据时递增版本号，并在更新操作时比较版本号是否一致。
- `时间戳机制`：记录数据的创建时间和最后修改时间，在更新操作时比较最后修改时间是否一致。

<span style="color:red">乐观锁的优势在于不需要显式地加锁和解锁，降低了锁的开销。但是在高并发环境下，由于并发冲突的可能性增加，会导致乐观锁的检测失败，需要进行重试或回滚操作。</span>>

<span style="color:red">在选择使用悲观锁还是乐观锁时，需要根据具体业务场景和并发访问的特点进行权衡和选择。一般来说，对于多写少读、并发冲突概率高的场景，可以选择悲观锁；而对于多读少写、并发冲突概率低的场景，可以选择乐观锁。另外，还可以结合两种锁机制进行使用，根据实际情况灵活选取。</span>>

## 13.SQL优化的一般步骤是什么,怎么看执行计划（explain),如何理解其中各个字段的含义。

1. sql优化三部曲：

- `定位和排查慢SQL`：通过慢查询日志或慢查询日志分析工具获取慢查询列表，找出可能存在问题的SQL语句。
- `执行计划分析`：使用关键字 `EXPLAIN` 查看SQL语句的执行计划。重点关注 `type`、`key`、`rows`、`filtered` 等字段，以定位SQL执行慢的原因。
- `使用 SHOW PROFILE 工具`：开启 `SHOW PROFILE`，获取SQL执行过程中的资源开销情况，如IO开销、CPU开销、内存开销等。帮助深入了解并分析问题所在。

2. 关于执行计划（explain）和各个字段的含义:

> 执行计划是数据库查询优化的重要工具，用于分析查询的执行过程和效率。在大多数数据库中，可以通过执行 `EXPLAIN` 或 `EXPLAIN PLAN` 命令来生成执行计划。

执行计划通常包含以下字段：

- `id`：查询操作的标识符，用于标识不同的查询步骤。
- `select_type`：查询类型，表示查询的方式和逻辑，例如 SIMPLE（简单查询）、PRIMARY（主键查询）、SUBQUERY（子查询）等。
- `table`：查询操作涉及的表名。
- `type`：访问类型（Access Type），表示查询使用的访问方法，例如 ALL（全表扫描）、INDEX（索引扫描）、RANGE（范围扫描）、JOIN（连接操作）等。一般来说，访问类型从最好到最差依次为：system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL。
- `possible_keys`：可能使用的索引名称列表。
- `key`：实际使用的索引名称，如果为 NULL，则表示未使用索引。
- `key_len`：索引的长度，单位为字节。
- `ref`：表示查询中使用到的索引的列或常量。
- `rows`：估计的扫描行数，表示执行查询时需要扫描的数据行数。
- `Extra`：额外信息，可能包括排序、临时表使用、文件排序等。

## 14.select for update 有什么含义，会锁表还是锁行还是其他

当使用 `SELECT FOR UPDATE` 语句时，它会在查询的数据行上设置行级别的排他锁。这种加锁机制是悲观锁的一种实现方式，因为它假设并发事务之间可能会互相干扰，所以在读取数据时就会对数据行进行加锁，以防止其他事务对这些数据行进行修改。

具体加锁的范围取决于是否使用了索引/主键：

1. 如果查询条件中没有使用索引/主键或者没有可用的索引，则会对整个表进行加锁。这被称为表锁。表锁会导致并发性能下降，因为其他事务需要等待锁的释放才能继续操作。
2. 如果查询条件中使用了索引/主键，并且索引可用，则只会对匹配的数据行进行加锁。这被称为行锁。行锁的使用更加灵活，可以减少并发冲突，提高并发性能。

行锁的粒度更小，可以更好地支持并发操作，但管理成本更高。而表锁的粒度较大，适用于较少并发更新的场景。

因此，在使用 `SELECT FOR UPDATE` 语句时，如果能够合理使用索引/主键来优化查询条件，可以更好地控制锁的范围，避免不必要的锁竞争和性能问题。需要注意的是，加锁可能会导致其他事务等待锁的释放，甚至可能发生死锁情况，所以在设计并发控制策略时需要谨慎考虑。

综上所述，`SELECT FOR UPDATE` 语句在查询数据的同时还会对查询结果进行加锁，它是一种悲观锁机制。加锁的范围取决于是否使用了索引/主键，选择合适的加锁粒度可以提高并发性能和数据一致性。

## 15.MySQL 事务的四大特性以及实现原理

MySQL事务的四大特性是：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。

1. `原子性（Atomicity）`：原子性指的是事务中的操作要么全部成功执行，要么全部回滚到事务开始前的状态，不存在部分执行的情况。MySQL通过使用事务日志和回滚日志来实现原子性。当事务提交时，将所有操作记录到事务日志中，如果事务失败或者回滚，可以通过回滚日志将数据恢复到事务开始前的状态。
2. `一致性（Consistency）`：一致性要求事务执行前后数据库的状态必须保持一致。这意味着事务中的操作必须符合预定义的约束和规则。MySQL通过使用约束、触发器和存储过程等机制来确保数据的一致性。
3. `隔离性（Isolation）`：隔离性指的是并发执行的事务之间相互隔离，每个事务在执行过程中感知不到其他事务的存在。MySQL使用锁和事务隔离级别来实现隔离性。通过使用锁来控制并发访问，确保事务之间不会相互干扰。而事务隔离级别定义了读取操作所能看到的数据的一致性和并发性。
4. `持久性（Durability）`：持久性指的是一旦事务提交成功，其对数据库的改变将永久保存，即使在系统崩溃或重启的情况下也能够恢复。MySQL通过使用事务日志和redo日志来实现持久性。事务日志记录了事务的操作，而redo日志用于在系统故障恢复时重新执行已经提交的事务，从而保证数据的持久性。

## 16.如果某个表有近千万数据，CRUD比较慢，如何优化。

1. 索引优化：

- 确保表上的相关列都有适当的索引。通过分析常用查询和过滤条件，创建合适的索引可以大幅提高查询性能。
- 注意避免过多的索引，因为过多的索引不仅占用存储空间，也会增加写操作的开销。

2. 分区分表：

- 如果数据表具有时间或其他可分区的属性，可以考虑对数据进行分区，将数据按照一定的规则分散到不同的物理表中，以减少单个表的数据量，提高查询和写入的效率。

3. 正确选择存储引擎：

- 根据实际需求选择合适的存储引擎。如InnoDB、MyISAM等，各自有不同的特点和适用场景。

4. 优化SQL语句：

- 对频繁执行的SQL语句进行优化，合理使用索引，尽量减少全表扫描和笛卡尔积等性能消耗高的操作。
- 可以考虑使用EXPLAIN语句分析查询计划，查看是否存在慢查询、索引不命中等问题。

5. 避免大事务：

- 当表中有大量数据需要处理时，尽量避免一次性执行过大的事务。可以考虑将大事务分解成多个小事务，以降低对数据库的负载。

6. 数据库参数调优：

- 根据实际情况调整数据库的相关参数，如缓冲区大小、线程数量等，以提高数据库的性能和并发处理能力。

7. 数据缓存和预加载：

- 合理使用缓存技术，如Redis、Memcached等，减少对数据库的直接访问。
- 考虑预加载常用数据的策略，将热点数据提前加载到内存中，从而加快查询速度。

8. 定期维护和优化：

- 定期进行数据库的维护工作，包括表优化、索引重建、数据清理等，以保持数据库的良好状态和性能。

## 17.如何写 sql 能够有效的使用到复合索引

1. `确保索引适用于查询条件`：查询条件中应包含复合索引的前缀列。复合索引是按顺序建立的，因此只有当查询条件涉及到复合索引的左侧列时，才能有效利用索引。如果查询条件没有涉及到复合索引的前缀列，索引将无效。
2. `避免在复合索引列上进行函数操作`：函数操作（如函数调用、运算、类型转换等）会使得索引无法使用，导致全表扫描。因此，在查询中应尽量避免在复合索引列上进行函数操作。如果必须使用函数，可以考虑为该列创建单独的索引。
3. `避免过滤较大范围的数据`：如果查询条件不具体，涉及到复合索引的列的选择性较低，那么数据库可能会选择不使用索引进行全表扫描。因此，尽量使用更具体的查询条件，以提高选择性，使得数据库能够有效地使用复合索引。
4. `谨慎选择复合索引顺序`：复合索引的列顺序很重要。应考虑经常同时使用的列，将其放在复合索引的前面。这样可以增加索引的选择性，使其更有效地过滤数据。
5. `考虑覆盖索引`：在查询中尽量使用覆盖索引，即索引中包含了查询所需的所有列。这样可以避免回表操作，减少磁盘IO，提高查询性能。
6. `避免创建冗余索引`：对于多个复合索引，确保它们互补而不是重叠。避免创建冗余索引，因为冗余索引会占用额外的存储空间，增加索引维护的成本。



## 18.mysql中in和exists的区别

1. 功能区别：

- `IN`：用于在主查询中判断某个列的值是否存在于子查询的结果集中。可以理解为将子查询的结果集作为一个值列表，在主查询中进行匹配。

- `EXISTS`：用于判断主查询中的某个条件是否存在于子查询的结果集中。可以理解为检查子查询是否返回了至少一行结果。

2. 执行方式区别：

- `IN`：对于每一行主查询，将子查询的结果集加载到内存中，然后与主查询的对应列进行比较。适用于子查询返回的结果集较小的情况。

- `EXISTS`：对于每一行主查询，只需要判断子查询是否返回了至少一行结果即可，不需要加载整个结果集到内存中，因此在某些情况下可能更高效。

3. 适用场景：

- `IN`：适用于需要进行值列表匹配的情况，比如查找满足某些特定值条件的数据。

- `EXISTS`：适用于仅需要判断子查询是否返回结果，而不需要具体结果集的情况，比如判断某个表中是否存在满足特定条件的数据。

## 19.数据库自增主键可能遇到什么问题

1. `冲突问题`：多个并行的插入操作可能导致主键冲突。当两个事务同时进行插入操作，并且自增主键的值相同时，就会发生冲突。这可能会导致某些插入操作失败或引发错误。
2. `间隙问题`：当有记录被删除时，自增主键会产生间隙。这可能会导致主键的不连续性，浪费了一些主键值。这在某些情况下可能会影响性能，特别是在需要大量插入和删除记录的表上。
3. `主键耗尽问题`：自增主键的取值范围是有限的，通常是一个整数类型。如果表中的记录数超过了自增主键的最大值，就会出现主键耗尽问题。这种情况下，需要采取措施来扩展主键的范围，例如使用更大的整数类型或其他方案。
4. `数据迁移问题`：在将数据从一个MySQL数据库迁移到另一个MySQL数据库时，自增主键可能会引起问题。如果目标数据库已存在相同的自增主键值，或者自增步长设置不同，就可能会导致冲突或序列不连续。

## 20.MVCC熟悉吗，它的底层原理？

> MVCC（多版本并发控制）是一种数据库并发控制的方法，用于在多个事务同时对数据库进行读写操作时，保证数据一致性和隔离性。

MVCC涉及以下几个关键知识点：

1. `事务版本号`：每个事务在开始时会分配一个唯一的事务ID，用于记录该行数据的修改时间点。
2. `隐藏列`：为了支持MVCC，数据库系统会在每张表中添加隐藏列来存储事务版本号，用于跟踪数据的修改历史。
3. `Undo Log（回滚日志）`：当事务进行更新或删除操作时，会将之前的数据版本保存到Undo Log中，以便其他正在进行读操作的事务仍可以访问旧版本的数据。
4. `Read View（读视图）`：每个事务在开始时创建一个Read View，包含事务启动时的事务ID以及其他已提交的事务ID，用于确定可见的数据版本。
5. `快照读`：MVCC中的读操作通常基于快照，避免读操作受到正在进行的写操作的影响。
6. `并发冲突检测`：MVCC需要检测并处理并发事务之间的冲突，如采用锁机制或乐观并发控制。
7. `垃圾回收`：为了避免数据库膨胀，MVCC需要进行垃圾回收，删除不再需要的旧版本数据。
8. `可见性规则`：MVCC定义了可见性规则确定事务对数据的读写可见性，如"读已提交"和"可重复读"。
9. `性能影响`：尽管MVCC提供了并发性和隔离性的优势，但会带来一定的性能开销，包括计算和存储资源的消耗。

## 21.数据库中间件了解过吗，sharding-jdbc，mycat？

1. `Sharding-JDBC` 是一个开源的轻量级数据库中间件，它提供了分库分表的功能。通过 Sharding-JDBC，可以将数据切分到多个数据库实例中进行存储和查询。它提供了简单易用的配置方式，并且与主流的关系型数据库兼容，如 MySQL、Oracle 等。Sharding-JDBC 的核心思想是通过拦截 JDBC 的 SQL 语句，并根据配置的规则将数据路由到相应的数据库实例上，从而实现水平扩展和性能优化。

2. `MyCAT（MySQL Cluster Autonomic Tuning）`也是一个开源的数据库中间件，专注于关系型数据库的分布式集群。它以 MySQL 为后端存储引擎，通过代理和路由的方式，将数据请求路由到不同的数据库节点上。MyCAT 支持分库分表、读写分离、容灾备份等功能，具有高可用性和可伸缩性的特点。它提供了自动分表规则的配置，可以根据业务需求灵活地管理数据的分片和迁移。

3. 这两个数据库中间件都可以帮助应用程序实现数据的分布式存储和查询，提高系统的扩展性、性能和可用性。选择使用哪个中间件取决于具体的业务需求和技术栈，包括数据规模、性能要求、数据一致性等因素。

## 22.MYSQL 的主从延迟，你怎么解决？

1. 主从复制的五个步骤

- `步骤一`：主库将更新事件写入binlog，包括update、insert、delete等操作。
- `步骤二`：从库发起连接并连接到主库。

- `步骤三`：主库创建一个binlog dump线程，将binlog的内容发送给从库。

- `步骤四`：从库启动后，创建一个I/O线程，读取主库传来的binlog内容，并写入到relay log中。

- `步骤五`：从库还会创建一个SQL线程，从relay log中读取内容，从Exec_Master_Log_Pos位置开始执行读取到的更新事件，并将更新内容写入到从库的数据库。

2. 主从同步延迟的原因

- `从服务器的读取线程限制`：从服务器只有一个读取binlog的线程，当并发更新操作很大时，读取线程可能无法及时处理所有的binlog日志，从而导致延迟。增加从服务器的读取线程数量或者优化读取线程的性能可以改善这个问题。
- `执行时间较长的SQL操作`：当从服务器执行一个耗时较长的SQL操作时，例如复杂查询或锁表操作，它会阻塞其他SQL语句的执行，从而导致未同步的binlog积压在主服务器上。在此情况下，您可以尝试优化慢查询，使用合适的索引或者调整查询逻辑以提高执行效率。
- `锁表操作`：如果某个SQL需要对表进行锁定，那么其他操作需要等待该锁释放后才能执行，这也会导致主服务器上的SQL积压。避免过多的锁表操作或者减少锁定表的时间可以改善延迟问题。
- `主服务器的负载过高`：如果主服务器的负载过高，无法及时处理所有的请求，也会导致未同步的binlog积压。您可以考虑优化主服务器的硬件性能、调整配置以及分散负载等方法来改善延迟问题。

3. 主从同步延迟的解决办法

- `调整数据库参数`：您可以修改sync_binlog和innodb_flush_log_at_trx_commit等参数以改善主服务器的性能。这些参数的调整可能会对数据的一致性和持久性产生一定影响，请在调整前确保了解各个参数的含义和潜在风险。
- `提升从服务器硬件性能`：通过更换更好的硬件设备或提升从服务器的计算能力、内存和磁盘性能，可以加快从服务器的数据处理和同步速度。
- `配置从服务器作为备份`：将其中一个从服务器设置为纯备份服务器，不提供查询，只负责同步数据和作为备份存储。这样可以减轻该从服务器的负载，提高同步效率。
- `增加从服务器数量`：增加从服务器的数量可以分散读取压力，同时提高系统的可扩展性和冗余性。但请注意，增加从服务器数量也会增加网络开销和管理复杂性，需要合理评估资源和成本。

## 23.说一下大表查询的优化方案

1. `索引优化`：合理创建索引是提高大表查询性能的关键。通过分析查询语句和访问模式，确定适当的索引策略。可以考虑创建覆盖索引、复合索引或者使用全文索引等方式，以减少查询的扫描范围和减小磁盘IO开销。
2. `分页查询`：对于需要返回大量数据的查询，考虑使用分页查询，而不是一次性返回所有结果。通过使用LIMIT和OFFSET子句限制返回的数据量，可以减少查询的响应时间和资源消耗。
3. `查询优化器和执行计划`：数据库的查询优化器会根据查询语句和表结构生成最优的查询执行计划。确保表的统计信息是最新的，以保证优化器能够做出正确的决策。可以通过分析执行计划，检查是否存在慢查询或者索引失效的情况，并做相应的调整。
4. `分区表`：对于特别大的表，可以考虑将其拆分为多个分区（按照某个列的值进行划分），以便提高查询性能。这样可以减少每次查询需要扫描的数据量，加快查询速度。
5. `数据归档和压缩`：根据业务需求，将旧的数据归档到其他存储介质，并进行压缩。这样可以减少大表的数据量，提高查询效率。
6. `缓存查询结果`：对于一些相对静态的查询结果，可以考虑使用缓存技术（如Redis）来存储查询结果，以减少对数据库的访问。
7. `调整服务器硬件和配置`：增加服务器的内存、CPU等硬件资源，优化数据库的相关参数配置（如查询缓存大小、连接数等），以提升整体性能。
8. `数据库分库分表`：如果以上优化措施无法满足需求，可以考虑将数据进行水平或垂直分割，将表拆分为多个更小的表，甚至将数据划分到不同的数据库中，以减少单个表的数据量。

## 24.什么是数据库连接池?为什么需要数据库连接池呢?

数据库连接池是一种管理数据库连接的技术，它维护着一组预先创建的数据库连接，并对这些连接进行管理和复用。当应用程序需要与数据库进行通信时，可以从连接池中获取一个可用的数据库连接，完成数据库操作后，再将连接释放回连接池供其他请求使用。

数据库连接池的主要目的是提高数据库访问的性能和效率，具体原因如下：

1. `连接的创建和销毁开销大`：建立和关闭数据库连接是一项开销较大的操作。如果每个数据库访问请求都需要创建和销毁连接，会消耗较多的系统资源和时间。连接池通过事先创建一定数量的连接并维护它们的生命周期，避免了频繁创建和销毁连接的开销。
2. `连接的复用`：连接池能够重复利用已经创建的连接，在多个数据库请求之间共享连接。这样可以避免频繁地进行连接的创建和认证过程，减少了系统开销，提高了数据库访问速度和响应时间。
3. `连接数的控制`：通过连接池，可以限制同时存在的连接数量，防止过多的数据库连接请求导致数据库服务器负载过高。连接池可以维持连接的最小和最大数量，并且可以根据实际需求动态调整连接数，以保持系统的稳定性和性能。
4. `连接的管理`：连接池可以对连接进行有效的管理，包括连接的可用性检测、连接的超时处理、异常连接的回收等。通过这些管理操作，可以保证连接的有效性、可靠性和稳定性。

<span style="color:red ">综上所述，数据库连接池通过预先创建、复用和管理数据库连接，减少了连接的创建和销毁开销，提高了数据库访问的性能和效率。它是一种重要的技术手段，用来优化数据库应用程序的性能，并充分利用数据库资源。</span>>

## 25.一条 SQL 语句在 MySQL 中如何执行的？

![image-20230723145015327](/images/ms/image-20230723145015327.png)

一条 SQL 语句的执行过程通常涉及以下步骤：

1. `权限检查`：MySQL 在执行 SQL 语句之前，会对用户进行权限检查。如果用户没有执行该语句的权限，将直接返回错误信息。
2. `查询缓存（仅适用于 MySQL 8.0 之前）`：MySQL 会先检查查询缓存，看是否存在与当前 SQL 语句完全匹配的缓存结果。如果存在缓存，MySQL 将直接返回缓存的查询结果，无需进一步执行查询操作。
3. `语法分析和语义检查`：MySQL 对 SQL 语句进行词法分析和语法分析，确保语句的语法正确性。然后进行语义检查，例如检查表名、列名是否存在，以及其他语义约束。
4. `查询优化和执行计划生成`：MySQL 使用查询优化器对给定的 SQL 语句生成最优的执行计划。查询优化器会考虑表的大小、索引使用情况、统计信息等因素，决定查询使用的索引、连接顺序等，以尽量提高查询性能。
5. `执行查询`：在生成了最优的执行计划后，MySQL 开始执行查询操作。这包括读取数据块、排序、聚合等操作。如果是更新操作，还包括写入数据和事务处理。
6. `权限验证`：在查询执行期间，MySQL 还会进行权限验证，确保用户仍然具有执行该操作的权限。如果权限验证失败，将返回错误信息。
7. `返回结果`：当查询执行完成后，MySQL 将查询的结果返回给客户端。如果是查询操作，将返回查询结果集；如果是更新操作，将返回受影响的行数或其他相关信息。

## 26.InnoDB引擎中的索引策略，了解过吗？

1. `覆盖索引`：覆盖索引是指创建一个包含了所有需要查询的列的索引，这样在查询时直接通过索引就能获取到所需的所有列数据，无需回表查找数据行。由于不需要进行额外的IO操作，覆盖索引可以显著提高查询性能。
2. `最左前缀原则`：当在InnoDB中创建联合索引时，按照最左前缀原则建立索引。这意味着索引会按照索引列顺序进行排序，并且只有在索引列的最左边的子集上的查询才能充分利用索引。例如，如果一个表上存在一个(a,b,c)的联合索引，那么该索引将被认为是(a)、(a,b)和(a,b,c)的三个索引。
3. `索引下推`：索引下推是MySQL 5.6引入的优化技术。它允许在进行索引遍历过程中，在索引中包含的字段上进行判断，并直接过滤掉不满足条件的记录，从而减少回表的次数。通过索引下推优化，可以更有效地使用索引，提高查询性能。

## 27.一条 sql 执行过长的时间，你如何优化，从哪些方面入手？

1. `优化SQL结构`：检查SQL语句是否存在多表关联或嵌套子查询等复杂结构，尽量简化SQL语句，去除不必要的冗余字段和操作。如果可能，考虑拆分表格或使用临时表来优化查询。
2. `优化索引结构`：通过分析查询条件和经常访问的列，确定是否需要适当添加索引。合理的索引可以加速数据检索，但是过多或不必要的索引可能会降低写操作的性能，需要权衡。
3. `分离/分表`：对于包含大量数据的表，可以考虑将其分离或进行分表操作。例如，对于交易流水表，可以按照时间或其他维度进行分表，以减少单表的数据量，提高查询性能。
4. `数据库主从分离`：通过数据库主从复制技术，将读写操作分离到不同的数据库实例上。这样可以提高读操作的并发性和整体性能，减轻主数据库的负载压力。
5. `使用EXPLAIN分析`：通过执行EXPLAIN命令，查看SQL语句的执行计划，了解底层的查询操作、索引使用情况等。根据EXPLAIN的结果，可以针对性地优化SQL语句、索引和表结构。
6. `查看MySQL执行日志`：检查MySQL的执行日志，分析是否存在其他方面的问题，如慢查询日志、错误日志等。从日志中可以获得有关查询性能、数据库连接和配置等方面的信息，有助于发现潜在的性能瓶颈和问题。

## 28.MYSQL数据库服务器性能分析的方法命令有哪些?

1. `SHOW STATUS`：该命令显示各种服务器状态变量的值，可以通过查看不同的状态变量来监控服务器性能。一些常用的状态变量包括：

   - Bytes_received和Bytes_sent：用于监控与服务器之间的流量，了解数据的传输情况。

   - Com_*：以Com_开头的状态变量表示服务器正在执行的命令的计数，例如Com_select、Com_insert等，可以了解不同类型操作的执行次数。

   - Created_*：以Created_开头的状态变量表示在查询执行期间创建的临时表和文件的计数，了解临时对象的创建情况。

2. `SHOW PROFILES`：该命令用于分析当前会话中SQL语句执行的资源消耗情况。通过使用SHOW PROFILES命令，可以获取SQL语句的执行时间、CPU时间、I/O操作次数等详细信息，有助于定位性能瓶颈。

3. `EXPLAIN`：该命令用于分析查询语句的执行计划，了解查询优化器如何选择执行计划，推荐索引的使用情况等。

4. `SET GLOBAL slow_query_log = 1`：启用慢查询日志，记录执行时间超过指定阈值的SQL语句，可以通过分析慢查询日志来发现潜在的性能问题。

5. `SHOW ENGINE InnoDB STATUS`：用于显示InnoDB存储引擎的详细状态信息，包括锁信息、事务信息、I/O操作等，有助于了解InnoDB引擎的性能情况。

6. `SHOW PROCESSLIST`：用于显示当前MySQL服务器上正在执行的进程列表，可以查看每个连接的状态、执行的SQL语句等，有助于了解服务器的负载情况和响应时间。

<span style="color:red">此外，还有一些第三方性能分析工具，如Percona Toolkit、pt-query-digest等，可以提供更详细和综合的性能分析报告。根据实际需求，可以选择适合的工具来进行性能分析。</span>

## 29.Blob 和text 有什么区别？

1. `存储方式`：Blob（Binary Large Object）用于存储二进制数据，例如图片、音频或视频文件等。Text用于存储文本数据，如长文本、文章内容等。
2. `字符编码`：Text类型的数据是以字符为单位进行存储的，并且支持多种字符编码，如UTF-8、Latin1等。Blob类型没有字符编码的概念，它是以字节为单位进行存储的。
3. `排序和比较`：Text类型的数据按照字符进行排序和比较，而Blob类型的数据按照字节进行排序和比较。
4. `最大长度限制`：在MySQL中，Text类型的最大长度是65,535个字符（约为64KB），而Blob类型的最大长度取决于存储引擎的设置，最大可以达到 4GB。
5. `索引和搜索`：在InnoDB存储引擎中，Text类型的字段可以创建索引，并可以使用全文搜索功能，而Blob类型的字段默认无法创建索引。

## 30.mysql里记录货币用什么字段类型比较好？

1. 精度和范围：货币涉及到金额计算，需要保持高精度和确定的范围。因此，较好的字段类型是DECIMAL。 DECIMAL可以提供可调整的精度和范围，可以确保数据的准确性和精确计算。
2. 数据存储方式：DECIMAL和NUMERIC类型的值在MySQL中以字符串形式存储，而不是作为二进制浮点数。这样可以避免由于浮点数运算可能导致的精度损失问题，确保金额数据的准确性。

## 31.Mysql中有哪几种锁，列举一下？

![image-20230723155219221](/images/ms/image-20230723155219221.png)

1. `表锁（Table Lock）`：表锁的粒度最大，对整个表进行加锁。优点是开销小，加锁速度快，但并发度最低。由于锁冲突概率高，可能导致多个事务之间频繁等待锁释放。不会出现死锁情况。
2. `行锁（Row Lock）`：行锁的粒度最小，针对数据表中的每一行记录进行加锁。行锁的开销相对较大，加锁速度较慢，但并发度最高。由于锁定粒度小，锁冲突的概率较低，可以允许多个事务同时对不同行进行读写操作。然而，行锁可能会导致死锁的发生，需要谨慎处理。
3. `页锁（Page Lock）`：页锁介于表锁和行锁之间，锁定的粒度是数据表中的一页数据。页锁的开销和加锁速度介于表锁和行锁之间，一般并发度也处于中等水平。与行锁类似，页锁也可能发生死锁情况。

## 32.Hash索引和B+树区别是什么？你在设计索引是怎么抉择的？

Hash索引和B+树索引有以下几点不同：

1. `范围查询`：B+树索引可以支持范围查询，即查询某一范围内的数据；而Hash索引无法进行范围查询，只能进行等值查询。
2. `联合索引`：B+树索引支持联合索引的最左侧原则，即在联合索引中，只有从左到右的连续子集才能被利用到索引；而Hash索引不支持联合索引。
3. `排序`：B+树索引支持基于索引进行排序，可以使用ORDER BY语句进行排序操作；而Hash索引不支持排序操作。
4. `查询效率`：在等值查询方面，Hash索引效率更高，因为它使用哈希函数直接计算出存储位置，无需遍历索引树。而B+树索引需要通过索引树的搜索过程来定位数据。
5. `模糊查询`：在使用LIKE进行模糊查询时，若模糊查询的模式是以通配符%开头的情况下，B+树索引可以利用索引进行优化；而Hash索引无法进行模糊查询。

在设计索引时，一般需要考虑以下几个因素：

1. `数据访问模式`：如果需要频繁进行范围查询、排序或模糊查询，那么B+树索引可能更适合；如果主要进行等值查询，且查询效率是最重要的考虑因素，那么Hash索引可能更适合。
2. `空间和性能要求`：B+树索引通常比Hash索引占用更多的存储空间，而Hash索引在特定情况下可以提供更高的查询效率。
3. `数据更新频率`：如果数据更新频繁，特别是插入操作较多，B+树索引的维护代价相对较低；而Hash索引在数据更新时需要重新计算哈希值，维护成本较高。
4. `数据完整性要求`：B+树索引可以保持数据的排序和唯一性，适合用于约束和查找重复记录；而Hash索引无法提供排序和唯一性约束。

## 33.mysql的内连接、左连接、右连接有什么区别？

它们之间的区别如下:

1. `内连接（Inner Join）`：内连接返回两个表中满足连接条件的行，即只返回两个表中共有的匹配行。如果某个表中的行在另一个表中没有匹配的行，则不会被返回。
2. `左连接（Left Join）`：左连接返回左表中的所有行，以及右表中与左表匹配的行。如果右表中没有与左表匹配的行，则返回NULL值。
3. `右连接（Right Join）`：右连接返回右表中的所有行，以及左表中与右表匹配的行。如果左表中没有与右表匹配的行，则返回NULL值。

简而言之：

- 内连接保留两个表中匹配的行，丢弃非匹配的行。
- 左连接保留左表中的所有行，同时将与左表匹配的右表行合并。
- 右连接保留右表中的所有行，同时将与右表匹配的左表行合并。

## 34.说说MySQL的基础架构图


主要组成部分包括：

1. `Client（客户端）`：与MySQL交互的应用程序，可以是命令行工具、Web应用、应用程序等。
2. `MySQL Proxy`：可选组件，用于在客户端和MySQL服务器之间进行连接管理、负载均衡、请求过滤等。
3. `Parser（解析器）`：解析器解析SQL语句，将其转换为内部数据结构。
4. `Planner（查询计划器）`：根据解析结果生成查询计划，决定如何执行查询。
5. `Optimizer（查询优化器）`：对查询计划进行优化，选择最佳的索引、表连接方式等进行查询优化。
6. `Execution（执行器）`：根据优化后的查询计划执行查询，涉及数据读取、锁管理、事务控制等操作。
7. `Storage（存储引擎）`：负责数据存储和检索，MySQL支持多个存储引擎，如InnoDB、MyISAM等。
8. `Cache（缓存）`：可选组件，用于缓存查询结果、表结构等，提高查询性能。
9. `Buffer（缓冲区）`：用于缓存磁盘和内存之间的数据交换，如重做日志缓冲区、查询缓冲区等。
10. `Disk（磁盘）`：MySQL数据实际存储在磁盘上，包括数据文件、索引文件、日志文件等。

## 35.什么是内连接、外连接、交叉连接、笛卡尔积呢？

1. `内连接（Inner Join）`：内连接是一种根据两个或多个表之间的共同列进行匹配，返回满足连接条件的行的查询操作。它只返回关联字段在两个表中都存在的记录，并忽略掉不匹配的记录。
2. `外连接（Outer Join）`：外连接是一种获取两个表中的所有记录的查询操作，同时根据连接条件获取符合条件的匹配行。如果一个表中的记录在另一个表中没有匹配，外连接会返回NULL值来表示缺失的数据。
3. `交叉连接（Cross Join）`：交叉连接是一种在查询中使用的操作，它生成两个表的笛卡尔积（即所有可能的组合），并返回结果集。交叉连接不需要连接条件。
4. `笛卡尔积（Cartesian Product）`：笛卡尔积是指两个表的每一行与另一个表的每一行进行组合，并返回包含所有组合的结果集。它是交叉连接的另一种称呼。

## 36.说一下数据库的三大范式

1. `第一范式（1NF）`：要求数据表中的每一列都是不可再分的基本数据项，即每一列都是原子性的。它要求每个属性具有原子性，不可再分。如果一个属性包含多个值，就需要将其拆分为多个单值属性。例如，一个包含联系人信息的表，如果一个字段存储了多个电话号码，就应该将电话号码拆分为独立的字段。
2. `第二范式（2NF）`：在满足第一范式的基础上，要求数据表中的非主键列必须完全依赖于主键，而不能依赖于主键的一部分。换句话说，一个表中的所有非主键列都应该直接依赖于整个主键，而不是仅依赖于主键的一部分。如果存在部分依赖，就需要将其剥离出来形成独立的表，并通过外键关联。
3. `第三范式（3NF）`：在满足第二范式的基础上，要求数据表中的非主键列之间不能存在传递依赖关系。换句话说，非主键列之间不能相互依赖或冗余。如果存在传递依赖，则应将其拆分为独立的表来消除冗余和数据不一致性。

## 37.Mysql 的binlog 有几种录入格式？分别有什么区别？

1. Statement格式：

- 基于SQL语句的级别记录，即将每个执行的SQL语句记录到binlog中。
- binlog中存储的是实际执行的SQL语句。
- 简单直观，节省存储空间，但在某些情况下可能出现非确定性结果。
- 可能存在复杂的函数、触发器等操作无法得到准确记录以进行复制。

2. Row格式：

- 基于行的级别记录，即将每个受影响的行的具体数据变化记录到binlog中。
- binlog中存储的是受到修改的行的完整副本。
- 完全记录了每行的变化，适用于任何类型的语句，能够产生精确的复制结果。
- 会占用更多的存储空间，对于大量插入或更新操作的数据库，会增加binlog的大小。

3. Mixed格式：

- 是Statement和Row的混合使用，MySQL根据具体的语句选择使用哪种格式。
- 大多数语句使用Statement格式，而一些语句会使用Row格式，例如涉及到非确定性函数或触发器的语句。
- 综合了Statement和Row的优点，提供了较好的性能和存储效率。

## 38.InnoDB引擎的4大特性,了解过吗

1. `插入缓冲（Insert Buffer）`：插入缓冲是一种用于提高写入性能的机制。当执行INSERT操作时，InnoDB会将新插入的数据先写入到插入缓冲区中，然后再周期性地将其合并到主索引页中。这种机制可以减少随机磁盘IO，提高写入效率。
2. `二次写（Double Write）`：二次写是一种用于保证数据页在发生崩溃时的一致性和完整性的机制。当执行写操作时，InnoDB首先将数据写入到一个预写日志（write-ahead log）中，然后再将数据刷新到磁盘上的数据页。这样即使在写入过程中发生崩溃，数据页的内容也可以从预写日志中恢复，避免数据损坏。
3. `自适应哈希索引（Adaptive Hash Index，AHI）`：自适应哈希索引是InnoDB引擎用来提高读取性能的一种优化技术。它会根据实际查询模式动态地创建和维护一个哈希索引，用于加速频繁的查询操作。AHI能够快速定位到需要的数据页，减少了随机IO的开销。
4. `预读（Read Ahead）`：预读是一种用于提高查询性能的机制。InnoDB引擎会在执行查询操作时，顺序地读取磁盘上的数据页，并将其缓存在内存中，以备后续的查询使用。这样可以减少访问磁盘的次数，提高查询的响应速度

## 39.索引有哪些优缺点？索引有哪几种类型？

优点：

1. `提高查询性能`：索引可以快速定位到需要查询的数据，减少了数据库的扫描和匹配操作，提高了查询的效率。
2. `加速排序`：索引可以按照特定的顺序进行排序，使得排序操作更加高效。
3. `加速连接`：如果表之间有连接关系，通过使用索引，连接操作的效率将得到提升。
4. `强制唯一性`：索引可以强制保证某列或列组合的唯一性，确保数据的完整性。

缺点：

1. `额外的存储空间`：索引需要占用额外的存储空间来存储索引数据结构，这会增加数据库的存储需求。
2. `增加写操作的开销`：对于每次的写操作（如插入、更新和删除），索引不仅需要更新数据本身，还需要更新索引结构，这会增加写操作的开销。
3. `维护索引的成本`：当数据发生改变时，索引也需要进行相应的维护操作，这会带来一定的性能开销。
4. `可能引发查询优化问题`：索引并不是万能的，有时候错误地使用或过多使用索引可能导致查询优化器选择了不合适的执行计划，降低了查询性能。

索引类型：

1. `主键索引`：主键索引用于唯一标识表中的每一行数据，每个表只能有一个主键索引。主键索引要求数据列不允许重复且不允许为NULL。
2. `唯一索引`：唯一索引要求数据列的值唯一，但允许为NULL。一个表可以有多个唯一索引。
3. `普通索引`：普通索引是最基本的索引类型，没有唯一性的限制，允许为NULL值。
4. `全文索引`：全文索引是一种用于对文本内容进行分词和搜索的索引类型，常用于搜索引擎等应用。
5. `覆盖索引`：覆盖索引是指查询所需要的列已经完全包含在索引中，不需要再去读取数据行，可以提高查询的性能。
6. `组合索引`：组合索引是由多个列值组成的索引，用于支持组合搜索。组合索引的效率通常大于索引合并（分别对多个列使用单列索引）的效率。

## 40.创建索引有什么原则呢？

1. `最左前缀匹配原则`：只有在查询条件中使用索引的最左边列，才能充分利用索引。
2. `频繁作为查询条件的字段才去创建索引`：对于经常用作查询条件的字段，创建索引可以提高查询效率。
3. `频繁更新的字段不适合创建索引`：频繁更新的字段会导致索引的频繁调整和维护，影响性能。
4. `索引列不能参与计算，不能有函数操作`：索引列要尽量保持简单，避免在索引列上进行复杂的计算或函数操作。
5. `优先考虑扩展索引，而不是新建索引，避免不必要的索引`：根据具体的查询需求和性能瓶颈，考虑扩展现有索引而不是创建新的索引。
6. `在 order by 或者 group by 子句中，创建索引需要注意顺序`：根据具体的排序需求，创建适当的索引以支持排序操作。
7. `区分度低的数据列不适合做索引列（如性别）`：区分度低表示该列的取值重复度高，索引对于过滤数据的效果不明显。
8. `定义有外键的数据列一定要建立索引`：外键关联的字段应该创建索引，以提高关联查询的效率。
9. `对于定义为 text、image 数据类型的列不要建立索引`：这些列的数据量较大且内容复杂，不适合创建索引。
10. `删除不再使用或者很少使用的索引`：定期检查数据库中的索引，删除不再使用或者几乎不会被使用的索引，以避免占用存储空间和增加维护成本。

## 41.创建索引的三种方式

1. 在执行 CREATE TABLE 语句时创建索引： 在创建表的同时，可以在列定义后面添加索引。这种方式通过在 CREATE TABLE 语句中指定索引类型和相关信息来创建索引。

   例如，在你提供的示例中，使用了关键词 KEY 和 USING BTREE 来创建名为 `idx_name` 的索引。这里的 `idx_name` 是索引的名称，`name` 是索引所在的列名。通过这种方式，可以直接在表创建的过程中定义并创建索引。

2. 使用 ALTER TABLE 命令添加索引： ALTER TABLE 命令用于修改已存在的表结构，包括添加、删除、修改列等操作。通过使用 ALTER TABLE 命令，可以在已有的表上添加索引。

   例如，要在名为 `employee` 的表上添加一个名为 `idx_name` 的索引，可以使用以下命令：

   ```mysql
   ALTER TABLE employee ADD INDEX idx_name (name);
   ```

   这样就在 `employee` 表的 `name` 列上创建了一个名为 `idx_name` 的索引。

3. 使用 CREATE INDEX 命令创建索引： CREATE INDEX 命令是用于创建索引的独立命令。通过使用 CREATE INDEX 命令，可以在已有的表上创建索引。

   例如，要在名为 `employee` 的表上创建一个名为 `idx_name` 的索引，可以使用以下命令：

   ```mysql
   CREATE INDEX idx_name ON employee (name);
   ```

   这样就在 `employee` 表的 `name` 列上创建了一个名为 `idx_name` 的索引。

## 42.百万级别或以上的数据，你是如何删除的？

1. `删除索引`： 在大规模删除数据之前，可以考虑先删除相关的索引。删除索引可以提高删除操作的速度。你可以使用 ALTER TABLE 或 DROP INDEX 命令删除相应的索引。例如：

   ```mysql
   ALTER TABLE table_name DROP INDEX index_name;
   ```

2. `批量删除无用数据`： 一旦索引被删除，可以使用 DELETE 或 TRUNCATE TABLE 命令批量删除无用的数据。根据你的需求和数据的特定条件，构造相应的删除语句并执行。例如：

   ```mysql
   DELETE FROM table_name WHERE condition;
   ```

   注意：在进行大规模数据删除操作时，请确保已经备份了需要保留的数据，并谨慎选择删除条件，以免删除了需要保留的数据。

3. `重新创建索引`： 在完成数据删除后，可以使用 CREATE INDEX 或 ALTER TABLE 命令重新创建索引。重新创建索引可以提高查询性能和数据检索速度。例如：

   ```mysql
   CREATE INDEX index_name ON table_name (column_name);
   ```

   或者使用 ALTER TABLE 命令添加索引：

   ```mysql
   ALTER TABLE table_name ADD INDEX index_name (column_name);
   ```

## 43.什么是最左前缀原则？什么是最左匹配原则？

1. `最左前缀原则`： 最左前缀原则是在创建复合索引时，将最常用的列放在索引的最左边，以提高查询效率。当查询条件中涉及到索引的列时，只有从最左边的列开始连续使用索引，才能使查询更加高效。如果查询条件没有按照索引的最左前缀顺序使用列，那么索引将无法被利用，导致性能下降。

   举例来说，假设有一个包含三列的表：A、B 和 C。如果我们经常根据 A 和 B 进行查询，那么创建一个复合索引 (A, B, C) 就符合最左前缀原则。这样，在查询时如果只根据 A 和 B 进行条件筛选，索引可以被充分利用，查询效率将会很高。

2. `最左匹配原则`： 最左匹配原则是指当使用复合索引进行查询时，只有按照索引的最左边连续列进行匹配，才能充分利用索引提高查询性能。换句话说，在查询条件中只使用了索引的最左边列和连续的列时，数据库会一直使用索引进行匹配，直到遇到不满足最左边列匹配条件的情况才停止匹配。

   举例来说，假设有一个复合索引 (A, B, C)。如果要查询条件是只根据 A 进行筛选，那么索引可以完全匹配。但是如果查询条件是只根据 B 或 C 进行筛选，那么索引无法完全匹配，只能部分利用索引进行匹配。根据最左匹配原则，查询效率会受到影响。

## 44.B树和B+树的区别，数据库为什么使用B+树而不是B树？

1. B树（B-Tree）： B树是一种自平衡的多路搜索树，它具有以下特点：

- B树的节点可以包含多个键和对应的值。
- B树的各级节点都具有相同的最大子节点数目，通常称为阶(order)。
- B树的节点分布在整棵树中，从根节点到叶子节点的距离相等，因此查找效率稳定。

2. B+树（B+ Tree）： B+树是基于B树的一种变体，它针对磁盘存储进行了优化，并在数据库系统中广泛使用。B+树相较于B树有以下不同之处：

- B+树的内部节点不存储数据记录，只存储键值信息和指向子节点的指针。
- 所有数据记录都存储在叶子节点中，形成一个有序链表。
- 叶子节点之间通过指针连接，方便范围查询和顺序遍历。

为什么数据库使用B+树而不是B树？ 数据库使用B+树作为索引结构有以下原因：

- 有利于减少磁盘I/O操作：由于B+树叶子节点存储了所有数据记录，因此在范围查询和顺序遍历时，只需要遍历叶子节点即可，减少了磁盘I/O操作次数，提高了查询效率。
- 提供更好的顺序访问性能：B+树的叶子节点形成有序链表，便于范围查询和顺序遍历操作。
- 具有更高的存储密度：由于B+树的内部节点不存储数据记录，可以容纳更多的索引键，提高了存储空间的利用率。

总结起来，B+树相对于B树在数据库中具有更优秀的性能特点，尤其对于范围查询、顺序遍历和磁盘I/O操作等场景，B+树能够提供更好的查询效率和存储效率，因此被广泛应用于数据库系统中。

## 45.覆盖索引、回表等这些，了解过吗？

1. `覆盖索引`： 覆盖索引是指一个查询可以直接从索引中获取所需的数据，而不需要再去读取实际的数据行。当查询的列包含在某个索引中时，通过使用覆盖索引可以显著提高查询的性能，因为它避免了额外的磁盘IO操作。在执行覆盖查询时，MySQL只需要读取索引页，并从中提取所需的数据，而无需访问实际的数据行。

   举个例子，假设有一个表格名为`users`，包含`id`、`name`和`age`三个列，其中`id`是主键，我们创建了一个索引（非唯一索引）：`CREATE INDEX idx_name ON users (name)`。如果我们执行如下查询：

   ```mysql
   SELECT id FROM users WHERE name = 'John';
   ```

   由于索引`idx_name`中已经包含了`name`列，数据库可以直接从该索引中获取到查询结果，而无需回表操作，从而提高了查询效率。

2. `回表`： 回表是指在使用非聚簇索引进行查询时，当需要返回的数据超出了索引所包含的列或者查询的条件不能完全通过索引过滤时，数据库需要再次根据索引中的行指针回到主键索引或聚簇索引中获取完整的数据行。回表操作会导致额外的磁盘IO，因此在性能优化上需要尽量避免回表。

   举个例子，假设有一个表格名为`orders`，包含`id`、`user_id`和`status`三个列，其中`id`是主键，我们创建了一个索引（非唯一索引）：`CREATE INDEX idx_user ON orders (user_id)`。如果我们执行如下查询：

   ```mysql
   SELECT id, status FROM orders WHERE user_id = 123;
   
   ```

   在这种情况下，由于查询需要返回`id`和`status`两列，而这两列不在索引`idx_user`中，MySQL需要再次根据索引的行指针回到主键索引或聚簇索引中进行回表操作，以获取完整的数据行。

   为了减少回表操作，可以考虑使用覆盖索引，即创建一个包含所需列的索引，使得查询可以从索引中获取到所有需要的数据，而无需回表操作。对于上述例子，可以考虑创建一个包含`user_id`、`id`和`status`三个列的联合索引来覆盖查询的需求：

   ```mysql
   CREATE INDEX idx_covering ON orders (user_id, id, status);
   ```

   这样，当执行上述查询时，MySQL就可以直接从覆盖索引中获取到`id`和`status`列的值，而无需再进行回表操作。

## 46.非聚簇索引一定会回表查询吗？

非聚簇索引并不一定会导致回表查询。回表查询的发生与查询语句所需的列和索引的覆盖情况有关。

如果一个查询只需要获取到非聚簇索引中已经包含的列，而不需要回到主键索引或聚簇索引中获取其他列的值，那么就可以通过非聚簇索引进行覆盖查询，而无需回表操作。这样可以提高查询性能，因为避免了额外的磁盘IO。

然而，在以下情况下，非聚簇索引会导致回表查询：

1. `查询语句需要返回的列不在非聚簇索引中`：如果查询需要获取的列不在非聚簇索引中，数据库会根据非聚簇索引的行指针回到主键索引或聚簇索引中获取缺失的列数据，这就造成了回表操作。
2. `查询语句的过滤条件无法完全使用非聚簇索引进行匹配`：如果查询语句的过滤条件无法通过非聚簇索引进行完全匹配，那么数据库需要回到主键索引或聚簇索引中获取满足条件的行数据。这也会导致回表操作。

## 47.联合索引是什么？为什么需要注意联合索引中的顺序？

联合索引是一种在数据库表中基于多个列进行索引的技术。它允许用户在多个列上创建索引，以提高查询效率和数据检索速度。

需要注意联合索引中的顺序是因为在InnoDB引擎中采用了最左前缀匹配原则。这意味着查询时只能利用索引的最左边的列进行搜索和过滤，而后续的列无法有效利用索引。因此，如果联合索引的顺序不合理，可能会导致索引失效，影响查询性能。

## 48.数据库隔离级别与锁的关系

数据库隔离级别和锁是密切相关的，隔离级别定义了事务之间相互隔离的程度，而锁则是实现隔离级别的一种机制。

常见的数据库隔离级别包括：读未提交（Read Uncommitted）、读已提交（Read Committed）、可重复读（Repeatable Read）和串行化（Serializable）。

在不同的隔离级别下，数据库会采取不同的锁策略来保证数据的一致性和并发的正确性。

- `读未提交`：最低的隔离级别，允许一个事务读取另一个事务尚未提交的数据，不需要使用锁来实现隔离。可能导致脏读（Dirty Read）问题。
- `读已提交`：要求一个事务只能读取到另一个已经提交的事务的数据，通过在读取期间对数据进行加锁或使用快照实现隔离。可以避免脏读问题，但可能出现不可重复读（Non-repeatable Read）问题。
- `可重复读`：要求一个事务在整个过程中所读取的数据都是一致的，即使其他事务对数据进行了修改。通过使用锁或多版本并发控制（MVCC）实现隔离。可以避免脏读和不可重复读问题，但可能出现幻读（Phantom Read）问题。
- `串行化`：最高的隔离级别，要求事务串行执行，保证事务之间完全隔离，使用锁来实现。可以避免脏读、不可重复读和幻读问题，但会降低并发性能。

在实际应用中，根据业务需求和对数据一致性和并发性要求的权衡，选择合适的隔离级别，并结合适当的锁策略来实现数据库的正确隔离和高效并发操作。

## 49.聊聊 InnoDb 的锁，什么是死锁？怎么解决？



![image-20230724131019876](/images/ms/image-20230724131019876.png)

### InnoDB的锁：

1. #### 共享/排他锁：

共享/排他锁是一种行级锁，主要应用在InnoDB存储引擎中，用于保证事务的隔离性和一致性[1](https://bing.com/search?q=mysql+共享%2f排他锁)[2](https://cloud.tencent.com/developer/article/1863124)[3](https://zhuanlan.zhihu.com/p/48127815)[4](https://blog.csdn.net/jiaqingShareing/article/details/81300259)[5](https://developer.aliyun.com/article/532412)[6](https://zhuanlan.zhihu.com/p/46502248)。

共享锁（Share Lock）又称读锁，简称 S 锁：一个事务获取了一个数据行的共享锁，其他事务能获得该行对应的共享锁，但不能获得排他锁，即一个事务在读取一个数据行的时候，其他事务可以并发读取数据，但不能对该数据行进行增删改，直到已释放所有共享锁。如果事务对某行数据加上共享锁之后，可进行读写操作；其他事务可以对该数据加共享锁，但不能加排他锁，且只能读数据，不能修改数据。共享锁可以使用 SELECT … LOCK IN SHARE MODE 语句来获取。

排他锁（eXclusive Lock）又称写锁，简称 X 锁：一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁（排他锁或者共享锁），即一个事务在读取一个数据行的时候，其他事务不能对该数据行进行增删改查。获取排他锁的事务既能读取数据，也能修改数据。排他锁可以使用 SELECT … FOR UPDATE 语句来获取。

对于 insert、update、delete 操作，InnoDB 会自动给涉及的数据加排他锁（X），只有查询 select 需要我们手动设置排他锁或者共享锁。所有被共享/排它锁查询所设置的锁都会在事务提交或者回滚之后被释放。

举个例子，假设有一个表 users(id PK, name)，其中 id 是主键。现在表中有四条记录：

| id   | name |
| :--- | :--- |
| 1    | 小罗 |
| 5    | 小黄 |
| 7    | 小明 |
| 11   | 小红 |

现在有两个事务 A 和 B 同时执行以下语句：

```sql
-- 事务 A
BEGIN;
SELECT * FROM users WHERE id = 1 FOR UPDATE; -- 加排他锁
SELECT SLEEP(10); -- 延迟10秒
UPDATE users SET name = '小新' WHERE id = 1; -- 修改数据
SELECT SLEEP(15); -- 延迟15秒
COMMIT;

-- 事务 B
BEGIN;
SELECT * FROM users WHERE id = 1; -- 普通查询
SELECT * FROM users WHERE id = 1 FOR UPDATE; -- 加排他锁
COMMIT;
```

这两个事务都会在 id 索引上加上排他锁，事务 A 的排他锁范围是 (1, 1]，事务 B 的排他锁范围也是 (1, 1]。因为这两个排他锁之间有冲突，所以事务 B 的加排他锁操作会被阻塞，直到事务 A 提交或回滚。但是普通查询不会被阻塞。

总之，mysql中的共享/排它锁是一种行级锁，通过在不同的查询语句后面添加 LOCK IN SHARE MODE 或者 FOR UPDATE 来获取。共享/排它锁可以保证事务的隔离性和一致性，并避免脏读、不可重复读和幻读现象的发生。

2. #### 意向锁：

意向锁是一种表级别的锁，它表示事务对表中的某些行有加锁的意图。意向锁有两种类型：意向共享锁（IS）和意向排他锁（IX）。意向共享锁表示事务想要对表中的某些行加共享锁（S），而意向排他锁表示事务想要对表中的某些行加排他锁（X）。

意向锁的作用是为了支持多粒度锁，即允许行级锁和表级锁共存。当一个事务想要对一个表加表级的共享锁或排他锁时，它不需要检查表中的每一行是否有其他事务持有排他锁，而只需要检查表上是否有意向排他锁。如果有，说明有其他事务对表中的某些行加了排他锁，那么这个事务就不能对整个表加共享锁或排他锁。如果没有，说明没有其他事务对表中的任何行加了排他锁，那么这个事务就可以对整个表加共享锁或排他锁。

意向锁之间是互相兼容的，也就是说多个事务可以同时对一个表加意向共享锁或意向排他锁。但是意向锁会与普通的表级共享锁或排他锁互斥，除了意向共享锁和共享锁之间是兼容的。注意，意向锁不会与行级的共享锁或排他锁互斥，这样可以保证多个事务对不同的行加排他锁时的并发性。

下面举一个例子来说明意向锁的作用：

假设有一个users表，其中有6条记录，id从1到6。

- 事务A先获取了id为6的行的排他锁，并未提交：

```sql
SELECT * FROM users WHERE id = 6 FOR UPDATE;
```

- 此时users表存在两把锁：users表上的**意向排他锁**和id为6的行上的**排他锁**。
- 之后事务B想要获取users表的共享锁：

```sql
LOCK TABLES users READ;
```

- 此时事务B检测到事务A持有users表的**意向排他锁**。
- 事务B对users表的加锁请求被阻塞（排斥）。
- 最后事务C也想获取users表中某一行的排他锁：

```sql
SELECT * FROM users WHERE id = 5 FOR UPDATE;
```

- 事务C申请users表的**意向排他锁**。
- 事务C检测到事务A持有users表的**意向排他锁**。
- 因为**意向锁之间并不互斥**，所以事务C获取到了users表的**意向排他锁**。
- 因为id为5的行上不存在任何**排他锁**，最终事务C成功获取到了该行上的**排他锁**。

3. #### 记录锁：

记录锁是一种行级别的锁，它锁定一条记录上的索引，阻止其他事务对该记录进行修改或删除。记录锁可以存在于主键索引或唯一索引中，通常是通过等值查询或主键范围查询获得的。例如：

```sql
SELECT * FROM t WHERE id = 1 FOR UPDATE;
```

这条语句会在id为1的记录上加上记录锁，其他事务不能对这条记录进行更新或删除操作。

记录锁的作用是为了保证数据的一致性和完整性，防止出现脏写或不可重复读的现象。当一个事务对一个记录加了记录锁后，它可以确保在事务提交之前，该记录不会被其他事务修改或删除。

记录锁之间是互斥的，也就是说多个事务不能同时对同一条记录加记录锁。但是记录锁可以与共享锁兼容，也就是说一个事务可以对一个已经被共享锁锁定的记录加记录锁，只要该共享锁来自同一个事务。例如：

```sql
SELECT * FROM t WHERE id = 1 LOCK IN SHARE MODE;
SELECT * FROM t WHERE id = 1 FOR UPDATE;
```

这两条语句都来自同一个事务，第一条语句会在id为1的记录上加上共享锁，第二条语句会在id为1的记录上加上记录锁，这样不会产生冲突。但是如果第二条语句来自另一个事务，那么就会被阻塞，直到第一个事务释放共享锁。

4. #### 间隙锁（Gap Lock）:

 记录锁是一种行级别的锁，它锁定一条记录上的索引，阻止其他事务对该记录进行修改或删除。记录锁可以存在于主键索引或唯一索引中，通常是通过等值查询或主键范围查询获得的。例如：

```sql
SELECT * FROM t WHERE id = 1 FOR UPDATE;
```

这条语句会在id为1的记录上加上记录锁，其他事务不能对这条记录进行更新或删除操作。

记录锁的作用是为了保证数据的一致性和完整性，防止出现脏写或不可重复读的现象。当一个事务对一个记录加了记录锁后，它可以确保在事务提交之前，该记录不会被其他事务修改或删除。

记录锁之间是互斥的，也就是说多个事务不能同时对同一条记录加记录锁。但是记录锁可以与共享锁兼容，也就是说一个事务可以对一个已经被共享锁锁定的记录加记录锁，只要该共享锁来自同一个事务。例如：

```sql
SELECT * FROM t WHERE id = 1 LOCK IN SHARE MODE;
SELECT * FROM t WHERE id = 1 FOR UPDATE;
```

这两条语句都来自同一个事务，第一条语句会在id为1的记录上加上共享锁，第二条语句会在id为1的记录上加上记录锁，这样不会产生冲突。但是如果第二条语句来自另一个事务，那么就会被阻塞，直到第一个事务释放共享锁。

5. #### 临键锁(Next-Key Lock):

[临键锁是一种组合锁，包括记录锁和间隙锁。记录锁是锁定索引记录本身，间隙锁是锁定索引记录之间的空隙。临键锁的作用是，防止其他事务在索引记录或间隙中插入、更新、删除数据，从而保证事务的可重复读和一致性](https://cloud.tencent.com/developer/article/1917462)[1](https://cloud.tencent.com/developer/article/1917462)[2](https://zhuanlan.zhihu.com/p/48269420)[3](https://blog.csdn.net/fofcn/article/details/123192969)[4](https://developer.aliyun.com/article/873074)[5](https://www.zhihu.com/question/587743193)。

临键锁的范围是一个左开右闭的区间，例如 (A, B] 表示锁定 A 和 B 之间的间隙以及 B 记录本身。临键锁的左边界是由查询条件向下寻找最靠近的索引值确定的，右边界是由查询条件向上寻找最靠近的索引值确定的。

临键锁只在使用非唯一索引进行数据检索时产生，如果使用唯一索引或主键进行检索，只会产生记录锁或间隙锁。临键锁也只在 RR 隔离级别下有效，如果使用 RC 隔离级别，只会产生记录锁。

举个例子，假设有一个表 users(id PK, name, age KEY)，其中 id 是主键，age 是普通索引。现在表中有三条记录：

| id   | name | age  |
| :--- | :--- | :--- |
| 1    | Mike | 10   |
| 2    | Jone | 20   |
| 3    | Tony | 30   |

现在有两个事务 A 和 B 同时执行以下语句：

```sql
-- 事务 A
SELECT * FROM users WHERE age = 20 FOR UPDATE;

-- 事务 B
INSERT INTO users VALUES (4, 'Bill', 25);
```

这两个事务都会在 age 索引上加上临键锁，事务 A 的临键锁范围是 (10, 20] 和 (20, 30]，事务 B 的临键锁范围是 (20, 25] 和 (25, 30]。因为这两个临键锁之间有冲突，所以事务 B 的插入操作会被阻塞，直到事务 A 提交或回滚。

总之，mysql中的临键锁是一种组合锁，通过在非唯一索引上锁定记录和间隙，可以保证事务的可重复读和一致性，并避免幻读现象的发生。

6. #### 插入意向锁

[插入意向锁是一种在 INSERT 操作之前设置的一种间隙锁，插入意向锁表示了一种插入意图，即当多个不同的事务，同时往同一个索引的同一个间隙中插入数据的时候，它们互相之间无需等待，即不会阻塞（要是单纯按照之前间隙锁的理论，必须要等一个间隙锁释放了，下一个事务才可以往相同的间隙处插入数据）](https://juejin.cn/post/7178321966024097829)[1](https://juejin.cn/post/7178321966024097829)[2](https://zhuanlan.zhihu.com/p/344542398)[3](https://blog.csdn.net/u012702547/article/details/128360647)[4](https://www.yisu.com/zixun/767091.html)[5](https://developer.aliyun.com/article/1233418)。

插入意向锁的作用是，提高并发插入的性能和兼容性，避免不必要的阻塞和死锁。插入意向锁是一种特殊的间隙锁，它不会与其他插入意向锁冲突，也不会影响其他事务对表的读取操作。但是，它会与其他类型的间隙锁或记录锁冲突，例如共享锁或排他锁。

举个例子，假设有一个表 users(id PK, name, age KEY)，其中 id 是主键，age 是普通索引。现在表中有三条记录：

| id   | name | age  |
| :--- | :--- | :--- |
| 1    | Mike | 10   |
| 2    | Jone | 20   |
| 3    | Tony | 30   |

现在有两个事务 A 和 B 同时执行以下语句：

```sql
-- 事务 A
INSERT INTO users SELECT 4, 'Bill', 15;

-- 事务 B
INSERT INTO users SELECT 5, 'Louis', 16;
```

这两个事务都会在 age 索引上加上插入意向锁，分别表示它们想要在 (10,20) 和 (15,20) 的间隙中插入数据。因为这两个插入意向锁之间不冲突，所以两个事务都可以继续执行，并且在 id 索引上加上记录锁。最终，表中会有五条记录：

| id   | name  | age  |
| :--- | :---- | :--- |
| 1    | Mike  | 10   |
| 2    | Jone  | 20   |
| 3    | Tony  | 30   |
| 4    | Bill  | 15   |
| 5    | Louis | 16   |

如果没有插入意向锁，那么两个事务就会在 age 索引上加上普通的间隙锁，这样就会导致一个事务阻塞另一个事务，降低并发性能。

总之，mysql中的插入意向锁是一种优化并发插入的机制，通过在索引的间隙上声明插入意图，可以提高性能和兼容性，并避免不必要的阻塞和死锁情况的发生。

7. #### 自增锁

自增锁是MySQL一种特殊的锁，如果表中存在自增字段，MySQL便会自动维护一个自增锁[1](https://www.cnblogs.com/detectiveHLH/p/14832940.html)[2](https://zhuanlan.zhihu.com/p/48207652)[。自增锁的作用是保证在并发插入下，自增列的值是正确和连续的](https://www.cnblogs.com/gaogao67/p/11123772.html)[3](https://www.cnblogs.com/gaogao67/p/11123772.html)。

[MySQL中有三种自增锁模式，分别是传统模式（Traditional）、连续模式（Consecutive）和交叉模式（Interleaved），它们对应配置项 innodb_autoinc_lock_mode 的值0、1、2](https://www.cnblogs.com/gaogao67/p/11123772.html)[3](https://www.cnblogs.com/gaogao67/p/11123772.html)[。不同的模式会影响自增锁的获取和释放时机，以及自增值的分配方式](https://www.cnblogs.com/gaogao67/p/11123772.html)[3](https://www.cnblogs.com/gaogao67/p/11123772.html)。

- [传统模式：所有的插入语句都会获取自增锁，并在语句执行完后释放。这样可以保证单个语句内生成的自增值是连续的，但是会降低并发性能](https://www.cnblogs.com/gaogao67/p/11123772.html)[3](https://www.cnblogs.com/gaogao67/p/11123772.html)。
- [连续模式：如果插入语句能够提前确定插入的数据量，则不会获取自增锁，否则还是会获取自增锁。这样可以提高部分插入语句的并发性能，但是无法保证单个语句内生成的自增值是连续的](https://www.cnblogs.com/gaogao67/p/11123772.html)[3](https://www.cnblogs.com/gaogao67/p/11123772.html)。
- [交叉模式：所有的插入语句都不会获取自增锁，而是使用较为轻量的互斥锁。这样可以最大化并发性能，但是无法保证单个语句内生成的自增值是连续的，也可能导致主从同步出现问题](https://www.cnblogs.com/gaogao67/p/11123772.html)[3](https://www.cnblogs.com/gaogao67/p/11123772.html)。

[在MySQL 8.0之前，默认的自增锁模式是连续模式，在MySQL 8.0之后，默认的自增锁模式是交叉模式](https://www.cnblogs.com/gaogao67/p/11123772.html)[3](https://www.cnblogs.com/gaogao67/p/11123772.html)[。这是因为MySQL 8.0之后，默认的Binlog格式从Statement变成了Row，这样可以避免交叉模式下主从同步出现主键ID不匹配的问题](https://www.cnblogs.com/gaogao67/p/11123772.html)[3](https://www.cnblogs.com/gaogao67/p/11123772.html)。

如果您想要更深入地了解MySQL中的自增锁，请参考以下链接：

- [深入剖析 MySQL 自增锁 - detectiveHLH - 博客园](https://www.cnblogs.com/detectiveHLH/p/14832940.html)
- [MySQL 的锁机制 - 自增锁 - 知乎](https://zhuanlan.zhihu.com/p/48207652)
- [MySQL AutoIncrement–自增锁模式 - TeyGao - 博客园](https://www.cnblogs.com/gaogao67/p/11123772.html)

### 什么是死锁，如何解决？

[MySQL中的死锁是指两个或两个以上的事务在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去](https://zhuanlan.zhihu.com/p/267522634)[1](https://zhuanlan.zhihu.com/p/267522634)[2](https://blog.csdn.net/AlbenXie/article/details/118613349)。例如，事务A占用了资源X，同时请求资源Y；事务B占用了资源Y，同时请求资源X。这样，事务A和B都在等待对方释放资源，形成了死锁。

MySQL有两种死锁处理方式[2](https://blog.csdn.net/AlbenXie/article/details/118613349)[3](https://blog.csdn.net/xiaowanzi_zj/article/details/119768235)：

- **等待**，直到超时（innodb_lock_wait_timeout=50s）。
- **发起死锁检测**，主动回滚一条事务，让其他事务继续执行（innodb_deadlock_detect=on)。

为了提高性能，一般都是使用死锁检测来进行处理死锁。

[MySQL中产生死锁的原因有很多，主要与事务的隔离级别、锁的类型、索引的设计、操作的顺序等有关](https://zhuanlan.zhihu.com/p/267522634)[1](https://zhuanlan.zhihu.com/p/267522634)[2](https://blog.csdn.net/AlbenXie/article/details/118613349)[4](https://www.cnblogs.com/hhthtt/p/10707541.html)。一些常见的死锁案例有：

- **间隙锁互斥**，导致锁等待。间隙锁是InnoDB在可重复读隔离级别下为了防止幻读而加的一种锁，它不锁定记录本身，而是锁定记录之间的空隙。如果一个事务删除某个区间内的一条不存在记录，获取到共享间隙锁，会阻止其他事务在相应的区间插入数据，因为插入需要获取排他间隙锁。
- **共享锁和排他锁互斥**，导致死锁。共享锁是为了保证读取数据的一致性而加的一种锁，它允许多个事务同时读取同一条记录；排他锁是为了保证写入数据的一致性而加的一种锁，它只允许一个事务写入同一条记录。如果两个或多个事务同时试图往表中插入一条相同主键或唯一键的记录，会发生共享锁和排他锁互斥的情况。
- **加锁顺序不一致**，导致死锁。如果两个或多个事务需要同时修改多个表或多个行的数据，而它们加锁的顺序不一致，就可能发生死锁。例如，事务A先修改表t1再修改表t2；事务B先修改表t2再修改表t1。这样就可能出现交叉等待的情况。

MySQL中解决死锁的方法有以下几种[1](https://zhuanlan.zhihu.com/p/267522634)[2](https://blog.csdn.net/AlbenXie/article/details/118613349)[4](https://www.cnblogs.com/hhthtt/p/10707541.html)：

- **收集死锁信息**：利用命令SHOW ENGINE INNODB STATUS查看死锁原因；调试阶段开启innodb_print_all_deadlocks，收集所有死锁日志。
- **减少死锁**：使用事务，不使用lock tables；保证没有长事务；尽量基于primary或unique key更新数据；操作完之后立即提交事务；如果在用(SELECT … FOR UPDATE or SELECT … LOCK IN SHARE MODE)，尝试降低隔离级别；修改多个表或者多个行的时候，将修改的顺序保持一致；创建索引，可以使创建的锁更少；最好不要用(SELECT … FOR UPDATE or SELECT … LOCK IN SHARE MODE)；如果上述都无法解决问题，那么尝试使用lock tables t1, t2, t3锁多张表。
- **处理死锁**：在应用程序中捕获死锁错误，重新执行被回滚的事务；或者使用存储过程，在存储过程中捕获死锁错误，重新执行被回滚的事务。

如果您想要更深入地了解MySQL中的死锁，请参考以下链接：

- [MySQL 死锁产生原因及解决方法 - 知乎](https://zhuanlan.zhihu.com/p/267522634)
- [彻底搞懂MySQL死锁 - CSDN博客](https://blog.csdn.net/AlbenXie/article/details/118613349)
- [MySQL模拟死锁以及分析和解决方案 - CSDN博客](https://blog.csdn.net/xiaowanzi_zj/article/details/119768235)

## 50.count(1)、count(*) 与 count(列名) 的区别？

count(1)、count(*) 与 count(列名) 都是SQL中的聚合函数，用于统计满足条件的记录数，但它们之间有一些不同点：

- **执行结果上**：count(1)和count(*****)的结果是一样的，都是统计满足条件的所有行数，不会忽略NULL值；count(列名)只统计满足条件的指定列的行数，会忽略NULL值[1](https://zhuanlan.zhihu.com/p/391861774)[2](https://zhuanlan.zhihu.com/p/410147647)[3](https://blog.csdn.net/weixin_44938368/article/details/109614917)。例如，如果表t有三行数据，其中name列有两个非空值和一个NULL值，那么count(1)和count(*)的结果都是3，而count(name)的结果是2。
- **执行效率上**：如果表有多个列并且没有主键，则count(1)的执行效率优于count(*****)；如果查询列为主键时，count(列名)的执行效率会优于count(1)，否则不如count(1)；如果表只有一个字段，则select count(*)最优[1](https://zhuanlan.zhihu.com/p/391861774)[2](https://zhuanlan.zhihu.com/p/410147647)[4](https://blog.csdn.net/A_dg_Jffery/article/details/107588525)。这是因为不同的函数在处理时会选择不同的索引或扫描方式，从而影响性能。

## 51.什么是存储过程？有哪些优缺点？

您好，这是Bing。我可以为您介绍一下存储过程的概念，以及它的优缺点。😊

[存储过程（Stored Procedure）是一组为了完成特定功能的SQL语句集，它经编译后存储在数据库中，用户通过指定存储过程的名字并给出参数（如果有的话）来执行它](https://cloud.tencent.com/developer/article/1767828)[1](https://cloud.tencent.com/developer/article/1767828)[2](https://zhuanlan.zhihu.com/p/87679196)[。存储过程可以增强SQL语句的功能和灵活性，也可以隐藏执行逻辑，只暴露名称和参数](https://cloud.tencent.com/developer/article/1767828)[1](https://cloud.tencent.com/developer/article/1767828)。

存储过程的优点有以下几个[1](https://cloud.tencent.com/developer/article/1767828)[2](https://zhuanlan.zhihu.com/p/87679196)[3](https://baike.tianqijun.com/doc/64948.html)：

- **提高执行效率**：存储过程是预编译的SQL语句集，执行时不需要再次编译，而且可以利用数据库的缓存机制，提高执行速度。
- **减少网络传输量**：客户端调用存储过程只需要传递存储过程名和参数，而不需要传递复杂的SQL语句，从而减少网络开销。
- **保证数据完整性**：存储过程可以将一系列处理步骤封装起来，保证数据在操作过程中的一致性和完整性。
- **增强使用安全性**：存储过程可以使没有权限的用户在控制之下间接地访问数据库，从而保证数据的安全。
- **实现集中控制**：存储过程可以实现业务逻辑的集中管理，当规则发生改变时，只需要修改存储过程就可以了。

存储过程的缺点有以下几个[1](https://cloud.tencent.com/developer/article/1767828)[2](https://zhuanlan.zhihu.com/p/87679196)[4](https://blog.csdn.net/qq_26963433/article/details/78762545)：

- **调试不方便**：存储过程没有现成的调试功能，如果出现错误或者逻辑问题，很难定位和修复。
- **创建权限受限**：创建存储过程需要有相应的权限，而不是所有用户都有这个权限的。
- **重新编译问题**：如果修改了表结构或者索引等，可能会导致存储过程失效或者效率降低，需要重新编译或者优化。
- **移植性问题**：不同数据库之间的存储过程可能存在语法或者功能上的差异，导致移植困难。

## 52.SQL 约束有哪几种呢？

[SQL约束（Constraints）是用于规定表中的数据规则的一种方法，如果存在违反约束的数据操作，操作会被终止](https://www.runoob.com/sql/sql-constraints.html)[1](https://bing.com/search?q=SQL+约束)[2](https://www.runoob.com/sql/sql-constraints.html)[。约束可以在创建表时指定（通过CREATE TABLE语句），或者在表创建之后指定（通过ALTER TABLE语句）](https://www.runoob.com/sql/sql-constraints.html)[1](https://bing.com/search?q=SQL+约束)[2](https://www.runoob.com/sql/sql-constraints.html)。

SQL中常见的约束有以下几种[1](https://bing.com/search?q=SQL+约束)[2](https://www.runoob.com/sql/sql-constraints.html)[3](https://www.w3school.com.cn/sql/sql_constraints.asp)：

- **NOT NULL**：指示某列不能存储NULL值，保证数据的完整性。
- **UNIQUE**：保证某列的每行必须有唯一的值，避免数据的重复。
- **PRIMARY KEY**：是NOT NULL和UNIQUE的结合，确保某列（或多个列的组合）有唯一标识，有助于更容易更快速地找到表中的一个特定的记录。
- **FOREIGN KEY**：保证一个表中的数据匹配另一个表中的值的参照完整性，建立两个表之间的关联。
- **CHECK**：保证列中的值符合指定的条件，限制数据的范围。
- **DEFAULT**：规定没有给列赋值时的默认值，简化数据的输入。

如果您想要更深入地了解SQL约束，请参考以下链接：

- [SQL 约束 | 菜鸟教程](https://www.runoob.com/sql/sql-constraints.html)
- [SQL 约束 (Constraints) - w3school 在线教程](https://www.w3school.com.cn/sql/sql_constraints.asp)
- [【SQL】数据库中的五种约束 - willingtolove - 博客园](https://www.cnblogs.com/willingtolove/p/9215330.html)

## 53.varchar(50)中 50 的涵义

- 字段最多存放 50 个字符 

- 如 varchar(50) 和 varchar(200) 存储 "jay" 字符串所占空间是一样的，后者在 排序时会消耗更多内存

## 54.mysql中int(20)和char(20)以及varchar(20)的区别

int(20)、char(20)和varchar(20)都是mysql中的数据类型，表示整数、固定长度字符串和可变长度字符串，但它们在存储方式和长度限制上有一些不同点：

- **int(20)**：表示整数类型，占用4个字节的存储空间，可以存放的范围是-2147483648到2147483647（如果是无符号的，则是0到4294967295）。括号中的20表示显示宽度，即在查询结果中显示的最大字符数，不影响实际存储的值[1](https://dev.mysql.com/doc/refman/8.0/en/char.html)[2](https://stackoverflow.com/questions/12126991/cast-from-varchar-to-int-mysql)。例如，如果插入一个值为123的整数，查询结果中会显示为00000000000000000123（共20个字符），但实际存储的值仍然是123。
- [**char(20)**：表示固定长度字符串类型，占用的存储空间取决于所使用的字符集](https://blog.csdn.net/u014143369/article/details/60386547)[3](https://blog.csdn.net/u014143369/article/details/60386547)[。例如，在UTF-8字符集中，一个英文字符占用一个字节，一个汉字占用三个字节。括号中的20表示最多可以存放20个字符，不足的部分会用空格填充](https://blog.csdn.net/u014143369/article/details/60386547)[3](https://blog.csdn.net/u014143369/article/details/60386547)[4](https://blog.csdn.net/sdlyjzh/article/details/81328481)。例如，如果插入一个值为’abc’的字符串，实际存储的值为’abc '（共17个空格）。
- [**varchar(20)**：表示可变长度字符串类型，占用的存储空间取决于实际输入的字符长度和所使用的字符集](https://blog.csdn.net/u014143369/article/details/60386547)[3](https://blog.csdn.net/u014143369/article/details/60386547)。括号中的20表示最多可以存放20个字符，不会用空格填充[3](https://blog.csdn.net/u014143369/article/details/60386547)[4](https://blog.csdn.net/sdlyjzh/article/details/81328481)[。例如，如果插入一个值为’abc’的字符串，实际存储的值为’abc’（无空格）。varchar类型在存储时会额外使用一个或两个字节来记录字符串的长度](https://developer.aliyun.com/ask/384390)[5](https://developer.aliyun.com/ask/384390)。

## 55.drop、delete 与truncate 的区别

|          | delete                                    | truncate                         | drop                                                   |
| :------- | :---------------------------------------- | :------------------------------- | :----------------------------------------------------- |
| 类型     | DML                                       | DDL                              | DDL                                                    |
| 回滚     | 可回滚                                    | 不可回滚                         | 不可回滚                                               |
| 删除内容 | 表结构还在，删除表的 全部或者一部分数据行 | 表结构还在， 删除 表中的所有数据 | 从数据库中删除表，所有的 数据行，索引和权限也会被 删除 |
| 删除速度 | 删除速度慢，逐行删除                      | 删除速度快                       | 删除速度最快                                           |

drop、delete 与truncate 都是mysql中的语句，用于删除表中的数据，但它们在执行方式和效果上有一些不同点：

- [**drop**：属于数据库DDL（数据定义语言），会隐式提交，所以不能回滚，不会触发触发器。drop语句删除表结构及所有数据，并将表所占用的空间全部释放。drop语句将删除表的结构所依赖的约束、触发器、索引等](https://zhuanlan.zhihu.com/p/270331768)[1](https://zhuanlan.zhihu.com/p/270331768)[2](https://www.cnblogs.com/zhizhao/p/7825469.html)。
- [**truncate**：属于数据库DDL（数据定义语言），会隐式提交，所以不能回滚，不会触发触发器。truncate语句删除表中所有记录，并且将重新设置高水线和所有的索引，缺省情况下将空间释放到minextents个extent，除非使用reuse storage](https://zhuanlan.zhihu.com/p/270331768)[1](https://zhuanlan.zhihu.com/p/270331768)[2](https://www.cnblogs.com/zhizhao/p/7825469.html)。truncate能够快速清空一个表，并且重置auto_increment的值[1](https://zhuanlan.zhihu.com/p/270331768)[2](https://www.cnblogs.com/zhizhao/p/7825469.html)。
- [**delete**：属于数据库DML（数据操作语言），执行delete操作时，每次从表中删除一行，并且同时将该行的的删除操作记录在redo和undo表空间中以便进行回滚（rollback）和重做操作，需要手动提交（commit）操作才能生效，可以通过rollback撤消操作。delete可根据条件删除表中满足条件的数据，如果不指定where子句，那么删除表中所有记录。delete语句不影响表所占用的extent，高水线保持原位置不变](https://zhuanlan.zhihu.com/p/270331768)[1](https://zhuanlan.zhihu.com/p/270331768)[2](https://www.cnblogs.com/zhizhao/p/7825469.html)。

## 56.UNION 与UNION ALL 的区别？

您好，这是Bing。我可以为您介绍一下UNION 与UNION ALL 的区别。😊

UNION 和 UNION ALL 都是mysql中的语句，用于把来自多个SELECT语句的结果组合到一个结果集合中，但它们在执行方式和效果上有一些不同点：

- [**UNION**：会把结果集中重复的记录删掉，相当于distinct，同时进行默认规则的排序。UNION 只能对表中具有相同字段属性的列进行合并](https://blog.csdn.net/A_Runner/article/details/91789648)[1](https://blog.csdn.net/A_Runner/article/details/91789648)[2](https://zhuanlan.zhihu.com/p/354723197)。
- [**UNION ALL**：会把所有的记录返回，不管是不是重复，不进行排序。UNION ALL 的效率高于 UNION，因为它不需要进行去重和排序的操作](https://blog.csdn.net/A_Runner/article/details/91789648)[1](https://blog.csdn.net/A_Runner/article/details/91789648)[2](https://zhuanlan.zhihu.com/p/354723197)。

## 57.SQL的生命周期

1. 服务器与数据库建立连接 
2. 数据库进程拿到请求 sql 
3. 解析并生成执行计划，执行 
4. 读取数据到内存，并进行逻辑处理 
5. 通过步骤一的连接，发送结果到客户端 
6. 关掉连接，释放资源

## 58.一条 Sql 的执行顺序？

- **FROM**：确定查询涉及的表或视图，以及它们之间的连接关系。
- **ON**：过滤掉不符合连接条件的记录。
- **JOIN**：根据连接类型（如内连接、外连接等）合并表或视图。
- **WHERE**：过滤掉不符合筛选条件的记录。
- **GROUP BY**：根据指定的列或表达式对记录进行分组。
- **WITH ROLLUP**：生成分组的汇总行。
- **HAVING**：过滤掉不符合分组条件的组。
- **SELECT**：确定查询返回的列或表达式，以及它们的别名。
- **DISTINCT**：去除重复的记录。
- **ORDER BY**：根据指定的列或表达式对记录进行排序。
- **LIMIT**：限制返回的记录数。

## 59.关心过业务系统里面的 sql 耗时吗？统计过慢查询吗？对慢查询都怎么优化过？

1. 我们平时写 Sql 时，都要养成用 explain 分析的习惯。
2. 慢查询的统计，运维会定期统计给我们

优化慢查询： 

- 分析语句，是否加载了不必要✁字段/数据。 
- 分析 SQl 执行句话，是否命中索引等。
- 如果 SQL 很复杂，优化 SQL 结构 
- 如果表数据量太大，考虑分表

## 60.主键使用自增 ID 还是 UUID，为什么？mysql 自增主键 用完了怎么办？

在选择主键类型时，应根据具体业务需求和系统设计考虑。下面是关于自增ID和UUID主键的一般性建议：

自增ID主键：

1. 优点：

- 简单：自增ID主键是数据库提供的机制，可以自动生成唯一的递增值，不需要开发者手动指定。
- 效率高：自增ID主键基于数值类型，对于索引、排序、连接等操作效率较高。
- 占用空间小：自增ID主键通常使用较少的存储空间，对于大型数据表和索引有优势。

2. 缺点：

- 不具备全局唯一性：自增ID主键只在当前表中保持唯一，如果跨表或跨系统需要全局唯一标识，可能会遇到冲突和管理问题。
- 无法提前生成：需要插入数据库后才能获取到自增ID的值。

UUID主键：

1. 优点：

- 全局唯一性：UUID（通用唯一标识符）是通过算法生成的，几乎可以保证全局唯一，适用于分布式系统和多表关联场景。
- 提前生成：UUID主键可以在应用层生成，而不需要插入数据库后获取，方便生成唯一标识。

2. 缺点：

- 占用空间大：UUID主键通常使用128位二进制表示，相对于自增ID主键使用的存储空间更大。
- 效率较低：由于UUID是随机生成的，索引效率比较低，对于大型数据表和索引可能会影响性能。

当自增ID主键用完后，可以考虑以下方案：

- 增加字段长度：如果自增ID使用的是整数类型，可以修改字段类型为长整型（BIGINT），这样能够提供更大的ID范围。
- 数据库迁移：可以将数据迁移到一个新的数据库中，该数据库的自增ID从更大的值开始。
- 使用UUID：可以迁移到使用UUID作为主键的数据库中，确保全局唯一性。
- 分区表：将原来的单表分割成多个子表，每个子表使用独立的自增ID主键。这样可以扩展ID范围，同时减轻单表的负载压力。
- 与业务方沟通：与业务方协商，了解是否有其他标识或者方案可以取代自增ID，并根据具体情况进行调整。

## 61.MySQL数据库cpu飙升的话，要怎么处理呢？

1. **使用操作系统命令（如top命令）观察是否是mysqld进程导致CPU占用高。如果不是，找出占用高的其他进程，并进行相关处理。**
2. **如果是mysqld进程导致CPU飙升，可以使用"show processlist"命令查看当前运行的会话情况，检查是否有消耗资源较高的SQL在运行。找出消耗高的SQL语句。**
3. **检查这些消耗高的SQL语句的执行计划是否准确，是否存在索引缺失等问题。可以使用"explain"命令来分析查询计划，确保查询语句能够高效执行。**
4. **根据具体情况，进行相应的调整。例如，可以考虑对缺失索引的表添加索引、对复杂的查询语句进行优化、适当调整内存参数等。**
5. **在进行调整之前，可以尝试"kill"掉消耗高的会话，观察CPU使用率是否下降。然后根据调整后的配置，重新执行这些SQL语句，检查CPU使用率是否得到改善。**

## 62.读写分离常见方案？

读写分离是一种提高数据库性能的常见技术，它可以将数据库的读操作和写操作分发到不同的实例上，从而减轻单个实例的压力。读写分离的基础是MySQL的主从复制，即一个主库负责写入，多个从库负责读取，从库通过复制主库的数据来保持数据一致性。

读写分离的常见方案有以下几种：

- **基于MySQL proxy代理的方式**：在应用和数据库之间增加代理层，代理层接收应用对数据库的请求，根据不同请求类型转发到不同的实例，在实现读写分离的同时可以实现负载均衡。这种方式对于应用来说是透明的，但是可能存在代理层的单点故障或性能瓶颈。开源方案有MySQL-proxy、cobar、mycat、Atlas等[1](https://zhuanlan.zhihu.com/p/86108084)。
- [**基于应用内路由的方式**：在应用程序中实现，针对不同的请求类型去不同的实例执行sql。这种方式灵活度高，但是也增加了应用逻辑的复杂度。可以基于spring的aop实现](https://zhuanlan.zhihu.com/p/86108084)[1](https://zhuanlan.zhihu.com/p/86108084)。
- [**基于mysql-connector-java的jdbc驱动方式**：在jdbc的url中配置主库和从库的地址，jdbc会自动将读请求发送给从库，将写请求发送给主库，同时还能实现多个从库的负载均衡。这种方式使用简单，但限制了需要使用java开发](https://zhuanlan.zhihu.com/p/86108084)[1](https://zhuanlan.zhihu.com/p/86108084)[2](https://blog.csdn.net/inrgihc/article/details/108293738)。
- [**基于sharding-jdbc的方式**：sharding-jdbc是sharding-sphere的核心模块，是一个强大的读写分离、分表分库中间件。它可以与springboot集成，通过配置化地实现读写分离](https://zhuanlan.zhihu.com/p/86108084)[1](https://zhuanlan.zhihu.com/p/86108084)[2](https://blog.csdn.net/inrgihc/article/details/108293738)。

## 63.MySQL的主从复制原理以及流程

主从复制原理，简言之，就三步曲，如下：

- **主数据库有个 bin-log 二进制文件，纪录了所有增删改 Sql 语句。（binlog 线程） ****
- **从数据库把主数据库的 bin-log 文件的 sql 语句复制过来。（io 线程） **

- **从数据库的relay-log 重做日志文件中再执行一次这些 sql 语句。（Sql 执行 线程）**

如下图所示：

**![image-20230726013824646](/images/ms/image-20230726013824646.png)**

上图主从复制分了五个步骤进行： 

- **步骤一**：主库的更新事件(update、insert、delete)被写到 binlog 

- **步骤二**：从库发起连接，连接到主库。 

- **步骤三**：此时主库创建一个 binlog dump thread，把 binlog 的内容发送到从库。 
- **步骤四**：从库启动之后，创建一个 I/O 线程，读取主库传过来的binlog 内容并写入到 relay log 
- **步骤五**：还会创建一个 SQL 线程，从 relay log 里面读取内容，从Exec_Master_Log_Pos 位置开始 执行读取到的更新事件，将更新内容写入到slave 的 db

## 64.Innodb 的事务实现原理？

1. `原子性`：是使用 undo log 来实现的，如果事务执行过程中出错或者用户执行 了rollback，系统通过 undo log 日志返回事务开始的状态。
2. `持久性`：使用 redo log 来实现，只要 redo log 日志持久化了，当系统崩溃，即可 通过redo log 把数据恢复。 
3. `隔离性`：通过锁以及 MVCC,使事务相互隔离开。 
4. `一致性`：通过回滚、恢复，以及并发情况下的隔离性，从而实现一致性。

## 65.谈谈 MySQL 的 Explain

[MySQL的Explain命令是用来分析SQL语句的执行计划的，它可以帮助我们优化SQL语句的性能。Explain命令可以用于SELECT, DELETE, INSERT, REPLACE, UPDATE等语句](https://zhuanlan.zhihu.com/p/281517471)[1](https://bing.com/search?q=MySQL+的+Explain)[2](https://zhuanlan.zhihu.com/p/281517471)。

使用Explain命令的格式如下：

```sql
EXPLAIN [options] statement
```

其中，options可以指定输出格式、分区信息等，statement是要分析的SQL语句。

Explain命令的输出结果是一个表格，每一行表示查询中访问的一个表或者子查询，每一列表示查询执行计划的一个属性。常见的列有：

- **id**：表示查询中每个select子句的序号，数字越大越先执行，相同数字表示从上到下依次执行。
- **select_type**：表示查询中每个select子句的类型，如SIMPLE（简单查询）、SUBQUERY（子查询）、DERIVED（派生表）、UNION（联合查询）等。
- **table**：表示当前行正在访问哪个表或者子查询，如果有别名则显示别名。
- **partitions**：表示当前查询匹配记录的分区，对于未分区的表，返回NULL。
- **type**：表示连接类型，如system（系统表）、const（常量表）、eq_ref（唯一索引扫描）、ref（非唯一索引扫描）、range（范围扫描）、index（全索引扫描）、ALL（全表扫描）等。一般来说，type越靠前性能越好。
- **possible_keys**：表示可能用到的索引，不一定实际使用。
- **key**：表示实际使用的索引，如果为NULL，则没有使用索引。
- **key_len**：表示实际使用的索引长度，越短越好。
- **ref**：表示哪些列或常量与key一起被使用。
- **rows**：表示估计要读取的行数，不一定准确，但可以作为判断性能的参考。
- **filtered**：表示按表条件过滤的行百分比，越高越好。
- **Extra**：表示一些额外的信息，如Using index（使用了覆盖索引）、Using temporary（使用了临时表）、Using filesort（使用了文件排序）等。

## 66.Innodb 的事务与日志的实现方式

- Innodb的事务与日志的实现方式主要是通过重做日志（redo log）和回滚日志（undo log）两种机制来保证事务的ACID特性。
- 重做日志用来实现事务的持久性（D），即当事务提交时，必须先将该事务的所有日志写入到重做日志文件中，以便在数据库崩溃后恢复数据。重做日志由内存中的重做日志缓冲（redo log buffer）和磁盘上的重做日志文件（redo log file）两部分组成。重做日志是以块（block）为单位进行存储的，每个块大小为512字节，并且有一个递增的日志序列号（LSN）。重做日志记录的是每个页的物理修改操作，例如插入、删除、更新等。
- 回滚日志用来实现事务的原子性（A）和一致性（C），即当事务失败或者被回滚时，可以根据回滚日志恢复数据到之前的状态。回滚日志也是由内存和磁盘两部分组成，内存中是回滚段（rollback segment），磁盘上是回滚段头部（rollback segment header）和插入缓冲位图（insert buffer bitmap）。回滚日志记录的是每行记录的逻辑修改操作，例如插入、删除、更新等。
- Innodb还利用锁机制来实现事务的隔离性（I），即控制不同事务之间的并发访问，防止数据不一致或者丢失。Innodb支持行级锁和表级锁两种锁模式，以及多种隔离级别，例如读未提交（read uncommitted）、读已提交（read committed）、可重复读（repeatable read）和串行化（serializable）。Innodb还实现了多版本并发控制（MVCC），即通过为每行记录保存一个创建版本号和一个删除版本号，来判断该行记录是否对当前事务可见。

以上是关于Innodb的事务与日志的实现方式的简要介绍。如果您想了解更多细节，请参考以下链接：

[1](https://bing.com/search?q=Innodb+的事务与日志的实现方式) https://blog.csdn.net/argleary/article/details/104189850

[2](https://zhuanlan.zhihu.com/p/48327345) https://www.yisu.com/zixun/263960.html

[3](https://blog.csdn.net/no_game_no_life_/article/details/106718599) **https://www.jianshu.com/p/93c292a06ebf**

## 67.你们数据库是否支持emoji表情存储，如果不支持，如何操作？

是的，我们的数据库支持emoji表情的存储。在数据库中存储和处理emoji表情时，需要确保以下几点：

1. 字符编码：确认数据库、表和列的字符集编码为utf8mb4。utf8mb4字符集支持四字节的Unicode字符，包括emoji表情。
2. 连接设置：在建立与数据库的连接时，确保连接客户端的字符集编码也是utf8mb4。
3. 字符串插入和读取：在插入和读取emoji表情时，确保使用utf8mb4编码处理字符串。

如果数据库不支持emoji表情存储，可以考虑以下两种操作方式：

1. 字符编码转换：将数据库字符集编码修改为utf8mb4。这通常需要修改数据库、表和列的字符集编码，并确保连接客户端字符集编码也为utf8mb4。
2. 编码替换：将emoji表情进行编码替换，存储为特定的文本格式，例如将emoji表情转换成十六进制编码或其他特定的字符。在读取时，再进行反向的解码替换操作。

这样，即便数据库原始不支持emoji表情存储，也能通过字符编码转换或编码替换的方式来实现存储和读取emoji表情的需求。

## 68.一个 6 亿的表 a，一个 3 亿的表 b，通过外键 tid 关联， 你如何最快地查询出满足条件的第 50000 到第 50200 中的这 200 条数据记录。

- [如果表a的tid是自增长且连续的，并且表b的id有索引，那么可以直接使用limit和offset来分页查询，例如：`select * from a,b where a.tid = b.id and a.tid > 500000 limit 200 offset 50000;`](https://blog.csdn.net/anmoyyh/article/details/77774510)[1](https://blog.csdn.net/anmoyyh/article/details/77774510)[2](https://blog.csdn.net/weixin_48502062/article/details/107169290)[3](https://blog.nowcoder.net/n/f61a91053ac64d648ee2e27a81fa8f3d) 这种方法的优点是简单快速，缺点是依赖于表a的tid的连续性。
- [如果表a的tid不是连续的，那么可以使用子查询和覆盖索引来优化查询效率，例如：`select * from b, (select tid from a limit 50000,200) a where b.id = a.tid;`](https://blog.csdn.net/anmoyyh/article/details/77774510)[1](https://blog.csdn.net/anmoyyh/article/details/77774510)[2](https://blog.csdn.net/weixin_48502062/article/details/107169290)[3](https://blog.nowcoder.net/n/f61a91053ac64d648ee2e27a81fa8f3d) 这种方法的优点是不依赖于表a的tid的连续性，缺点是需要为表a和表b建立合适的索引。
- [如果表a和表b都很大且分布不均匀，那么可以考虑使用分区或者分表来减少扫描范围和并行处理，例如：`select * from a_part1,b_part1 where a_part1.tid = b_part1.id and a_part1.tid > 500000 limit 200 offset 50000 union all select * from a_part2,b_part2 where a_part2.tid = b_part2.id and a_part2.tid > 500000 limit 200 offset 50000;`](http://www.2cto.com/database/201311/254385.html)[4](http://www.2cto.com/database/201311/254385.html) 这种方法的优点是可以利用多核处理能力，缺点是需要对表进行分区或者分表，并且维护相应的元数据。

## 69.Mysql 一条 SQL 加锁分析

一条 SQL 加锁，可以分 9 种情况：

- 组合一：`id列是主键，RC隔离级别` 

  在RC隔离级别下，读取操作不会加锁，因此对于读取操作不会存在加锁问题。对于写操作，MySQL会在执行时自动为涉及到的行加上共享锁，确保其他事务不会同时修改这些行。

- 组合二：`id列是二级唯一索引，RC隔离级别` 

  与组合一相似，在RC隔离级别下，读取操作不会加锁。对于写操作，MySQL会自动为涉及到的行加上共享锁。

- 组合三：`id列是二级非唯一索引，RC隔离级别` 

  在RC隔离级别下，读取操作不会加锁。对于写操作，MySQL会根据具体的条件判断是否需要加锁。如果写操作会涉及到索引上的记录，MySQL会为这些记录加上共享锁。

- 组合四：`id列上没有索引，RC隔离级别` 

  在RC隔离级别下，读取操作不会加锁。对于写操作，MySQL会根据具体的条件判断是否需要加锁。如果写操作会涉及到整个表或某些行，MySQL会为这些行加上表级写锁。

- 组合五：`id列是主键，RR隔离级别` 

  在RR隔离级别下，读取操作会在事务开始时为涉及到的行加上共享锁，并在事务结束时释放。对于写操作，MySQL会在执行时为涉及到的行加上排他锁，确保其他事务不能同时读取或修改这些行。

- 组合六：`id列是二级唯一索引，RR隔离级别` 

  与组合五相似，在RR隔离级别下，读取操作会在事务开始时为涉及到的行加上共享锁。对于写操作，MySQL会为涉及到的行加上排他锁。

- 组合七：`id列是二级非唯一索引，RR隔离级别` 

  在RR隔离级别下，读取操作会在事务开始时为涉及到的行加上共享锁。对于写操作，MySQL会根据条件判断是否需要加锁。如果写操作会涉及到索引上的记录，MySQL会为这些记录加上排他锁。

- 组合八：`id列上没有索引，RR隔离级别` 

  在RR隔离级别下，读取操作会在事务开始时为涉及到的行加上共享锁。对于写操作，MySQL会根据具体的条件判断是否需要加锁。如果写操作会涉及到整个表或某些行，MySQL会为这些行加上表级写锁。

- 组合九：`Serializable隔离级别` 

  在Serializable隔离级别下，读取操作和写操作都会为涉及到的行加上排他锁，保证事务之间的完全隔离性。在这个隔离级别下，可能会出现更多的锁冲突和并发性能下降问题。

## 70.Mysql如何解决幻读问题

幻读是指在一个事务中，同一个查询语句在不同的时间执行，得到不同的结果集，就好像有幻影一样。幻读通常是由于其他事务插入或删除了满足查询条件的记录所导致的。MySQL有四种事务隔离级别，其中只有最高的序列化级别可以完全避免幻读，但是会牺牲并发性能。MySQL的默认隔离级别是可重复读，它可以通过以下两种方式解决幻读：

- [针对快照读（普通select语句），是通过MVCC（多版本并发控制）方式解决了幻读。MVCC是一种在每行记录上保存多个版本的机制，每个版本有一个创建版本号和一个删除版本号，表示该行记录在哪个事务中被创建或删除。当一个事务要读取一行记录时，它会根据自己的版本号和该行记录的版本号来判断是否可见，从而保证在同一个事务中多次读取同样的结果](https://blog.csdn.net/m0_71777195/article/details/126968432)[1](https://blog.csdn.net/m0_71777195/article/details/126968432)[2](https://zhuanlan.zhihu.com/p/145822414)[3](https://zhuanlan.zhihu.com/p/360254683)。
- [针对当前读（select…for update等语句），是通过next-key lock（记录锁+间隙锁）方式解决了幻读。next-key lock是一种在索引上加锁的方法，它不仅锁定索引对应的记录，还锁定索引之间的间隙，防止其他事务在该间隙中插入新的记录](https://blog.csdn.net/m0_71777195/article/details/126968432)[1](https://blog.csdn.net/m0_71777195/article/details/126968432)[2](https://zhuanlan.zhihu.com/p/145822414)[4](https://www.cnblogs.com/wudanyang/p/10655180.html)。

# Redis专栏面试题

## 1.什么是 Redis？它主要用来什么的？

1. Redis是一个基于Key-Value存储结构的NoSQL开源内存数据库。
2. Redis提供了五种常用的数据类型：字符串（String）、哈希（Map）、集合（Set）、有序集合（ZSet）和列表（List）。
3. 不同的数据结构适用于解决不同的场景和问题，比如top10问题、好友关注列表、热点话题等。
4. Redis基于内存存储，并且在数据结构上进行了优化，因此具有出色的IO性能。
5. 在应用开发中，可以将Redis作为应用与数据库之间的分布式缓存组件使用，提升数据IO效率。
6. Redis是非关系型数据存储，不存在表之间的关联查询问题，进一步提升了数据IO效率。
7. Redis提供了主从复制加上哨兵模式以及集群方式实现高可用性。
8. Redis集群通过哈希槽的方式实现数据分片，进一步提升了性能。

## 2.说说 Redis 的基本数据结构类

### Redis的五种基本数据类型

#### String（字符串）

- `简介`:String 是 Redis 最基础的数据结构类型，它是二进制安全的，可以存储图片或者序列化的对象，值最大存储为 512M
- `简单使用举例`: set key value、get key等
- `应用场景`：共享 session、分布式锁，计数器、限流。
- `内部编码`:有 3 种，int（8字节长整型）/embstr（小于等于 39字节字符串）/raw（大于 39个字节字符串)

#### Hash（哈希）

- `简介`：在 Redis 中，哈希类型是指 v（值）本身又是一个键值对（k-v）结构  
- `简单使用举例`：hset key field value、hget key field 
- `内部编码`：ziplist（压缩列表） 、hashtable（哈希表）
- `应用场景`：缓存用户信息等。 
- `注意点`：如果开发使用 hgetall，哈希元素比较多✁话，可能导致 Redis 阻塞， 可以使用 hscan。而如果只是获取部分 field，✁议使用 hmget。

#### List（列表） 

- `简介`：列表（list）类型是用来存储多个有序✁字符串，一个列表最多可以存储 2^32-1 个元素。 
- `简单实用举例`： lpush key value [value ...] 、lrange key start end 
- `内部编码`：ziplist（压缩列表）、linkedlist（链表） 
- `应用场景`： 消息队列，文章列表

list 应用场景参考以下：

- lpush+lpop=Stack（栈）
- lpush+rpop=Queue（队列）
- lpsh+ltrim=Capped Collection（有限集合）
- lpush+brpop=Message Queue（消息队列)

#### Set（集合）

- `简介`：集合（set）类型也是用来保存多个✁字符串元素，但是不允许重复元素
- `简单使用举例`：sadd key element [element ...]、smembers key
- `内部编码`：intset（整数集合）、hashtable（哈希表）
- `注意点`：smembers 和 lrange、hgetall 都属于比较重的命令，如果元素过多存在阻塞Redis的可能性，可以使用 sscan 来完成。
- `应用场景`： 用户标签,生成随机数抽奖、社交需求。

#### 有序集合（zset）

- `简介`：已排序的字符串集合，同时元素不能重复
- `简单格式举例`：zadd key score member [score member ...]，zrank key member  
- `底层内部编码`：ziplist（压缩列表）、skiplist（跳跃表）
- `应用场景`：排行榜，社交需求（如用户点赞）。

### Redis 的三种特殊数据类型

- `BitMap（位图）`：BitMap 使用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。它适用于二值状态统计的场景，例如统计签到、登录次数等。常用命令有 setbit（设置位的值），getbit（获取位的值），bitcount（统计值为 1 的位数）等。
- `HyperLogLog（基数统计）`：HyperLogLog 是一种用来估算集合中不重复元素个数的数据类型。它可以用来统计网页的 UV（unique visitor）、在线用户数等。常用命令有 pfadd（添加元素），pfcount（获取近似基数）等。
- `GEO（地理位置）`：GEO 是一种用来存储和查询地理位置信息的数据类型。它可以用来实现基于位置的服务，例如打车、外卖软件中附近的 taxi 和餐馆的功能。常用命令有 geoadd（添加位置），georadius（查询范围内的位置），geodist（计算两点间的距离）等。

## 3.Redis 为什么这么快？

![image-20230728200020210](/images/ms/image-20230728200020210.png)

Redis是一种基于内存的高性能键值数据库，它的速度之所以这么快，有以下几个原因：

- Redis是单线程的，它不需要处理多线程之间的竞争和切换，也不需要加锁和解锁，节省了CPU的时间。
- Redis的数据结构是专门设计的，它们支持高效的读写操作，比如哈希表、跳表、压缩列表等。
- Redis使用了I/O多路复用技术，它可以同时监听多个套接字，并根据事件类型调用相应的处理器，避免了无效的轮询和等待。
- Redis运行在内存中，内存的访问速度远高于磁盘，而且Redis避免了磁盘I/O的开销。

综上所述，Redis之所以这么快，是因为它利用了内存、数据结构、单线程和I/O多路复用等技术，使得它的性能达到了极致。

## 4.什么是缓存击穿、缓存穿透、缓存雪崩？

### 4.1.缓存穿透问题

**先来看一个常见的缓存使用方式：**读请求来了，先查下缓存，缓存有值命中， 就直接返回；缓存没命中，就去查数据库，然后把数据库的值更新到缓存，再返回读取缓存

`缓存穿透`：指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库 查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到 数据库去查询，进而给数据库带来压力。

通俗点说，读请求访问时，缓存和数据库都没有某个值，这样就会导致每次对 这个值✁查询请求都会穿透到数据库，这就是缓存穿透。

**缓存穿透一般都是这几种情况产生的：**

- 业务不合理的设计，比如大多数用户都没开守护，但是你的每个请求都去缓存，查询某个 userid 查询有没有守护。

- 业务/运维/开发失误的操作，比如缓存和数据库的数据都被误删除了。

- 黑客非法请求攻击，比如黑客故意捏造大量非法请求，以读取不存在✁业务数据。

**如何避免缓存穿透呢？ 一般有三种方法:**

1.如果是非法请求，我们在 API 入口，对参数进行校验，过滤非法值。

2.如果查询数据库为空，我们可以给缓存设置个空值，或者默认值。但是如有写请求进来的话，需要更新缓存哈，以保证缓存一致性，同时，最后给缓存设置适当的过期时间。（业务上比较常用，简单有效）

3.使用布隆过滤器快速判断数据是否存在。即一个查询请求过来时，先通过布隆过滤器判断值是否存在，存在才继续往下查。

### 4.2.缓存雪崩问题

`Redis 缓存雪崩`是指在同一时间段，大量的缓存 key 同时失效或者 Redis 服务宕机，导致大量请求直接访问数据库，造成数据库的压力过大甚至崩溃的现象。

缓存雪崩的原因可能有以下几种：

- 缓存数据设置了相同的过期时间，导致在某一时刻缓存集中失效。
- Redis 服务器发生故障或者网络中断，导致缓存不可用。
- 缓存数据被误删除或者清空，导致缓存空洞。

针对缓存雪崩的问题，可以采用以下几种解决方案：

- [为缓存设置不同的过期时间，避免同一时间大量的缓存失效。可以在固定的过期时间上加上一个随机值，比如 5 小时 + 0 到 1800 秒](https://zhuanlan.zhihu.com/p/100706208)[1](https://zhuanlan.zhihu.com/p/100706208)[2](https://segmentfault.com/a/1190000041408356)[3](https://cloud.tencent.com/developer/article/1666384)。
- [搭建缓存集群，提高缓存的可用性和容灾能力。如果某个缓存服务器宕机，可以通过其他的缓存服务器提供服务](https://zhuanlan.zhihu.com/p/100706208)[1](https://zhuanlan.zhihu.com/p/100706208)[2](https://segmentfault.com/a/1190000041408356)。
- [使用熔断机制，当流量达到一定的阈值时，直接返回系统繁忙或者默认值，避免过多的请求打在数据库上](https://zhuanlan.zhihu.com/p/100706208)[1](https://zhuanlan.zhihu.com/p/100706208)[2](https://segmentfault.com/a/1190000041408356)。
- [提高数据库的性能和容灾能力，可以使用分库分表，读写分离，主从复制等策略](https://zhuanlan.zhihu.com/p/100706208)[1](https://zhuanlan.zhihu.com/p/100706208)[2](https://segmentfault.com/a/1190000041408356)。

### 4.3.缓存击穿问题

`缓存击穿`：指热点key 在某个时间点过期✁时候，而恰好在这个时间点对这个 Key 有大量的并发请求过来，从而大量的请求打到 db。

缓存击穿看着有点像，其实它两区别是，缓存雪奔是指数据库压力过大甚至 down 机，缓存击穿只是大量并发请求到了 DB 数据库层面。可以认为击穿是 缓存雪奔的一个子集吧。有些文章认为它俩区别，是区别在于击穿针对某一热 点 key 缓存，雪奔则是很多 key。 

解决方案就有两种： 

1.使用互斥锁方案。缓存失效时，不是立即去加载 db 数据，而是先使用某些带成 功返回的原子操作命令，如(Redis ✁ setnx）去操作，成功的时候，再去加载 db数据库数据和设置缓存。否则就去重试获取缓存。 

2. “永不过期”，是指没有设置过期时间，但是热点数据快要过期时，异步线程去 更新和设置过期时间。

### 4.4.解决以上三个问题的代码

```java
@Component
@Slf4j
public class CacheClient {
    private final RedisTemplate redisTemplate;
    @Autowired
    private RedissonClient redissonClient;

    public CacheClient(RedisTemplate redisTemplate) {
        this.redisTemplate = redisTemplate;
    }


    /**
     * 利用互斥锁解决缓存穿透问题设置空值
     *
     * @param cacheKeyPrefix 缓存key前缀
     * @param lockKeyPrefix  缓存锁key前缀
     * @param id             查询id
     * @param clazz          实体类
     * @param dbFallback     回调函数
     * @param time           过期时间值
     * @param timeUnit       时间单位
     * @param <R>
     * @param <ID>
     * @return
     */
    public <R, ID> R queryWithPassThrough(String cacheKeyPrefix, String lockKeyPrefix, ID id, Class<R> clazz, Function<ID, R> dbFallback, Long time, TimeUnit timeUnit) {
        String cacheKey = cacheKeyPrefix + id;
        //1.从redis查询商铺缓存
        String json = (String) redisTemplate.opsForValue().get(cacheKey);
        //2.判断是否存在
        if (StrUtil.isNotBlank(json)) {
            //3.存在直接返回
            R r = JSONUtil.toBean(json, clazz);
            return r;
        }
        if (json != null) {
            return null;
        }
        // 获取互斥锁并设置超时时间
        String lockKey = lockKeyPrefix + id;
        String lockValue = UUID.randomUUID().toString();
        Boolean acquired = redisTemplate.opsForValue().setIfAbsent(lockKey, lockValue, time, timeUnit);
        if (acquired != null && acquired) {
            try {
                // 重新检查缓存
                json = (String) redisTemplate.opsForValue().get(cacheKeyPrefix + id);
                if (StrUtil.isNotBlank(json)) {
                    return JSONUtil.toBean(json, clazz);
                } else {
                    //4.不存在根据id查询数据库
                    R r = dbFallback.apply(id);
                    //5.不存在返回错误
                    if (ObjectUtil.isNull(r)) {
                        redisTemplate.opsForValue().set(cacheKey, "", RedisConstants.CACHE_NULL_TTL, TimeUnit.SECONDS);
                        return null;
                    }
                    //6.存在写入redis
                    redisTemplate.opsForValue().set(cacheKey, JSONUtil.toJsonStr(r), RedisConstants.CACHE_TTL, TimeUnit.MINUTES);
                    //7.返回
                    return r;
                }
            } finally {
                // 释放互斥锁
                if (lockValue.equals(redisTemplate.opsForValue().get(lockKey))) {
                    redisTemplate.delete(lockKey);
                }
            }
        } else {
            // 未获取到锁，等待一段时间后重试
            try {
                Thread.sleep(100);  // 可根据实际情况调整等待时间
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
            // 递归调用自身重新尝试获取数据
            return queryWithPassThrough(cacheKeyPrefix, lockKeyPrefix, id, clazz, dbFallback, time, timeUnit);
        }
    }

    /**
     * 互斥锁解决缓存击穿代码
     *
     * @param cacheKeyPrefix 缓存key前缀
     * @param lockKeyPrefix  缓存锁key前缀
     * @param id             查询id
     * @param clazz          实体类
     * @param dbFallback     回调函数
     * @param time           过期时间值
     * @param timeUnit       时间单位
     * @param <R>
     * @param <ID>
     * @param retryInterval  线程等待毫秒数
     * @return maxRetryCount 最大重试次数
     */
    public <R, ID> R queryWithMutex(String cacheKeyPrefix, String lockKeyPrefix, ID id, Class<R> clazz, Function<ID, R> dbFallback, Long time, TimeUnit timeUnit, long retryInterval, int maxRetryCount) {
        String cacheKey = cacheKeyPrefix + id;
        String json = (String) redisTemplate.opsForValue().get(cacheKey);
        String lockKey = lockKeyPrefix + id;
        R r = null;
        boolean isLock = tryLock(lockKey);
        if (isLock) {
            try {
                r = dbFallback.apply(id);
                if (r == null) {
                    redisTemplate.opsForValue().set(cacheKey, "", RedisConstants.CACHE_NULL_TTL, TimeUnit.SECONDS);
                    return null;
                }
                redisTemplate.opsForValue().set(cacheKey, JSONUtil.toJsonStr(r), time, timeUnit);
            } catch (Exception e) {
                throw new RuntimeException(e);
            } finally {
                unlock(lockKey);
            }
        } else {
            try {
                Thread.sleep(retryInterval);
            } catch (InterruptedException e) {
                throw new RuntimeException(e);
            }
            if (maxRetryCount > 0) {
                return queryWithMutex(cacheKeyPrefix, lockKeyPrefix, id, clazz, dbFallback, time, timeUnit, retryInterval, maxRetryCount - 1);
            }
        }


        return r;
    }

    //加锁
    private boolean tryLock(String lockKey) {
        String lockValue = UUID.randomUUID().toString();
        Boolean flag = redisTemplate.opsForValue().setIfAbsent(lockKey, lockValue, 10, TimeUnit.MINUTES);
        return BooleanUtil.isTrue(flag);
    }

    //解锁
    private void unlock(String lockKey) {
        System.out.println("删除:" + lockKey);
        redisTemplate.delete(lockKey);
    }

    private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10);

    /**
     * 逻辑过期解决缓存击穿
     *
     * @param cacheKeyPrefix
     * @param lockKeyPrefix
     * @param id
     * @param clazz
     * @param dbFallback
     * @param time
     * @param timeUnit
     * @param <R>
     * @param <ID>
     * @return
     */
    public <R, ID> R queryWithLogicalExpire(String cacheKeyPrefix, String lockKeyPrefix, ID id, Class<R> clazz, Function<ID, R> dbFallback, Long time, TimeUnit timeUnit) throws ExecutionException, InterruptedException {
        String cacheKey = cacheKeyPrefix + id;
        String lockKey = lockKeyPrefix + id;
        // 1.从redis查询商铺缓存
        String json = (String) redisTemplate.opsForValue().get(cacheKey);
        R r = null;
        RLock lock = redissonClient.getLock(lockKey);
        // 2.判断是否存在
        if (StrUtil.isBlank(json)) {
            r = rebuildCach(id, dbFallback, time, timeUnit, lock, cacheKey);
        } else {
            // 4.命中，需要先把json反序列化为对象
            RedisData redisData = JSONUtil.toBean(json, RedisData.class);
            r = JSONUtil.toBean((JSONObject) redisData.getData(), clazz);
            LocalDateTime expireTime = redisData.getExpireTime();
            // 5.判断是否过期
            if (expireTime.isAfter(LocalDateTime.now())) {
                // 5.1.未过期，直接返回信息
                System.out.println("未过期，走缓存");
                return r;
            } else {
                // 5.2.已过期，需要缓存重建
                // 6.缓存重建
                // 6.1.获取互斥锁
                System.out.println("已过期");
                r = rebuildCach(id, dbFallback, time, timeUnit, lock, cacheKey);
            }
        }
        // 6.4.返回过期的商铺信息
        return r;
    }

    private <R, ID> R rebuildCach(ID id, Function<ID, R> dbFallback, Long time, TimeUnit timeUnit, RLock lock, String cacheKey) {
        System.out.println("重建缓存");
        boolean isLock = lock.tryLock();
        R r = null;
        // 6.2.判断是否获取锁成功
        if (isLock) {
            // 6.3.成功，开启独立线程，实现缓存重建
            try {
                R newR = dbFallback.apply(id);
                r = newR;
                CACHE_REBUILD_EXECUTOR.submit(() -> {
                    try {
                        //重建缓存
                        this.setWithLogicalExpire(cacheKey, newR, time, timeUnit);
                    } catch (Exception e) {
                        throw new RuntimeException(e);
                    }
                });

            } catch (Exception e) {
                throw new RuntimeException();
            } finally {
                lock.unlock();
            }

        }
        return r;
    }


    public void setWithLogicalExpire(String key, Object value, Long time, TimeUnit unit) {
        // 设置逻辑过期
        RedisData redisData = new RedisData();
        redisData.setData(value);
        redisData.setExpireTime(LocalDateTime.now().plusSeconds(unit.toSeconds(time)));
        // 写入Redis
        redisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(redisData));
    }

}
```

## 5.什么是热 Key 问题，如何解决热 key 问题

### 5.1.什么是热key问题？

热Key问题是指在Redis中，某个Key的访问频率远远高于其他的Key，导致大量的请求集中在某个Redis实例上，造成该实例的性能下降，甚至宕机。热Key问题可能会引起缓存击穿、请求分片不均、网络拥塞等危害。

### 5.2.热点key产生的主要原因：

- 用户消费的数据远大于生产的数据，如秒杀、热点新闻等读多写少的场景；
- 请求分片集中，超过单台Redis服务器的性能极限，比如固定名称Key，Hash落入同一台服务器，瞬间访问量极大；
- 业务规划不足、Redis不正确的使用、无效数据的堆积等，造成Key的Value过大或者Key中的成员数量过多；
- 预期外的访问量陡增，如突然出现的爆款商品、访问量暴涨的热点新闻、直播间某大主播搞活动带来的大量刷屏点赞、游戏中某区域发生多个工会间的战斗涉及大量玩家等。

### 5.3.如何解决热点key问题

- Redis集群扩容：增加分片副本，分摊客户端发过来的读请求；
- 将热Key分散到不同的服务器中：可以通过给热Key加上前缀或后缀，使其散列到不同的分片上，然后随机选择一个Key进行访问；
- 使用二级缓存：即在客户端使用JVM本地缓存或其他框架，减少对Redis的读请求；
- 使用读写分离：通过主从复制的方式，增加从节点来实现读请求的负载均衡。

## 6.Redis 过期策略和内存淘汰策略

![image-20230731184021000](/images/ms/image-20230731184021000.png)

**Redis 过期策略**是指当 Redis 中缓存的 key 过期了，Redis 如何处理。Redis 的过期策略有两种：定期删除和惰性删除。

- `定期删除`，指的是 Redis 默认是每隔 100 毫秒就随机抽取一些设置了过期时间的 key，检查是否过期，如果过期就删除。这种方式可以减少 CPU 的负载，但是可能会导致一些过期的 key 没有及时被删除。
- `惰性删除`，指的是当客户端访问一个 key 的时候，Redis 会检查这个 key 是否过期，如果过期就删除，不会返回任何数据。这种方式可以保证过期的 key 不会占用内存，但是可能会导致一些不常访问的 key 长时间占用内存。

**Redis 内存淘汰策略**是指当 Redis 的内存不足以容纳新写入数据时，Redis 如何选择哪些 key 进行淘汰。Redis 的内存淘汰策略有以下几种：

- `noeviction`：当内存不足以容纳新写入数据时，新写入操作会报错，不会驱逐任何键。这种方式可以保证数据的完整性，但是可能会导致服务不可用。
- `allkeys-lru`：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key。这种方式可以尽量保留热点数据，但是可能会导致一些重要但不常用的数据被淘汰。
- `volatile-lru`：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key。这种方式可以保证永久数据不被淘汰，但是可能会导致过期时间较长的数据被淘汰。
- `allkeys-random`：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key。这种方式可以简单快速地进行淘汰，但是可能会导致热点数据被淘汰。
- `volatile-random`：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。这种方式可以保证永久数据不被淘汰，但是可能会导致过期时间较短的数据被淘汰。
- `volatile-ttl`：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。这种方式可以尽量保留有效时间较长的数据，但是可能会导致有效时间较短但频繁访问的数据被淘汰。

## 7.说说 Redis 的常用应用场景

- `缓存`：缓存是Redis最常见的应用场景，之所以这么使用，主要是因为Redis读写性能优异，而且支持数据持久化和过期策略。缓存可以提高热点数据的访问速度，减轻数据库的压力，提升用户体验。例如，短信验证码、配置信息、商品详情、全页缓存等，都可以用Redis来缓存。
- `数据共享`：因为Redis是分布式的独立服务，可以在多个应用之间共享数据。例如，分布式Session、用户信息、购物车等，都可以用Redis来实现数据共享。
- `分布式锁`：利用Redis的单线程和原子性特性，可以实现分布式锁的功能。分布式锁可以保证多个并发请求对共享资源的互斥访问，避免数据不一致的问题。例如，秒杀、抢红包、库存扣减等，都可以用Redis来实现分布式锁。
- `全局ID`：利用Redis的原子性自增特性，可以实现全局ID的生成。全局ID可以用于分库分表、订单号、用户ID等场景。例如，incrby userid 1000 可以一次性拿一段ID。
- `计数器`：利用Redis的原子性自增或自减特性，可以实现计数器的功能。计数器可以用于统计某些指标或事件的发生次数或数量。例如，文章的阅读量、微博点赞数、网站访问量等，都可以用Redis来实现计数器。
- `限流`：利用Redis的原子性自增特性和过期策略，可以实现限流的功能。限流可以用于防止某些请求或用户对系统造成过大的压力或影响。例如，对用户的IP或其他信息作为key，每次访问增加一次计数，超过次数则返回false。
- `位统计`：利用Redis的位图（bitmap）数据结构和相关操作，可以实现位统计的功能。位统计可以用于大数据量的统计分析，节省空间和提高效率。例如，在线用户统计、留存用户统计、签到打卡等，都可以用Redis来实现位统计。
- `购物车`：利用Redis的字符串或哈希数据结构和相关操作，可以实现购物车的功能。购物车可以用于保存用户浏览或购买商品的信息。例如，key为用户ID，field为商品ID，value为商品数量。
- `用户消息时间线`：利用Redis的列表数据结构和相关操作，可以实现用户消息时间线的功能。用户消息时间线可以用于展示用户最新或最热门的动态或消息。例如，微博、微信朋友圈等。
- `消息队列`：利用Redis的列表数据结构和相关操作，可以实现消息队列的功能。消息队列可以用于异步处理任务或事件，提高系统吞吐量和可靠性。例如，订单产生后发货的数据、日志收集等。
- `抽奖`：利用Redis的集合数据结构和相关操作，可以实现抽奖的功能。抽奖可以用于随机选择一些幸运用户或者奖品。例如，spop myset 可以随机弹出集合中的一个元素。
- `点赞、签到、打卡`：利用Redis的集合数据结构和相关操作，可以实现点赞、签到、打卡等功能。这些功能可以用于记录用户的行为或喜好，增加用户的互动和粘性。例如，用 like:t1001 来维护 t1001 这条微博的所有点赞用户，用 checkin:u3001 来维护 u3001 这个用户的所有签到日期。
- `商品标签`：利用Redis的集合数据结构和相关操作，可以实现商品标签的功能。商品标签可以用于描述商品的特点或分类，方便用户搜索或浏览。例如，用 tags:i5001 来维护 i5001 这个商品的所有标签。
- `商品筛选`：利用Redis的集合数据结构和相关操作，可以实现商品筛选的功能。商品筛选可以用于根据用户的需求或偏好，筛选出符合条件的商品。例如，根据品牌、系统、屏幕尺寸、屏幕材质等条件，筛选出符合条件的手机。
- `用户关注、推荐模型`：利用Redis的集合数据结构和相关操作，可以实现用户关注、推荐模型的功能。用户关注、推荐模型可以用于建立用户之间的社交关系，提高用户的活跃度和满意度。例如，用 follow:u3001 来维护 u3001 这个用户关注的所有用户，用 fans:u3001 来维护 u3001 这个用户的所有粉丝，用 sinter 1:follow 2:fans 来查找两个人共同关注的用户。
- `排行榜`：利用Redis的有序集合数据结构和相关操作，可以实现排行榜的功能。排行榜可以用于展示某些指标或事件的排名情况，激发用户的竞争和参与意识。例如，用 hotNews:20190926 来维护 2019年9月26日 的热门新闻排行榜。

## 8.Redis 持久化有哪几种方式，怎么选？

Redis 持久化有两种方式，一种是基于快照的 RDB，另一种是基于日志的 AOF。这两种方式各有优缺点，怎么选取决于你的业务需求和场景。下面我简单介绍一下这两种方式的特点和适用场合：

- `RDB 持久化`是指 Redis 定期或手动地将内存中的数据以二进制格式保存到硬盘上的一个文件中，这个文件是一个完整的数据快照，可以用于备份、迁移或恢复数据。RDB 持久化的优点是文件体积小，恢复速度快，适合做全量备份或灾难恢复。缺点是数据不能做到实时或秒级持久化，可能会丢失最后一次快照后的数据，而且快照过程可能会造成性能下降或阻塞。
- `AOF 持久化`是指 Redis 将每个写命令都记录到一个只追加的文件中，这个文件保存了恢复数据集所需的所有操作。AOF 持久化的优点是数据可以做到实时或秒级持久化，丢失数据的风险小，而且文件格式易读易修复。缺点是文件体积大，恢复速度慢，而且重写过程可能会造成性能下降或阻塞。

根据这些特点，你可以根据以下几个方面来选择合适的持久化方式：

- `数据安全性`：如果你对数据的安全性要求很高，不能容忍任何数据丢失，那么你应该选择 AOF 持久化，并且设置为每次写入都同步到硬盘。如果你可以容忍少量数据丢失，那么你可以选择 RDB 持久化或者设置为每秒同步一次的 AOF 持久化。
- `数据备份和恢复`：如果你需要定期备份数据或者进行灾难恢复，那么你应该选择 RDB 持久化，因为它生成的文件更小更快。如果你需要实时备份数据或者进行部分恢复，那么你应该选择 AOF 持久化，因为它记录了所有的操作历史。
- `系统性能`：如果你对系统性能要求很高，不希望持久化过程影响服务质量，那么你应该选择 RDB 持久化，并且尽量减少快照频率。如果你对系统性能要求不高，可以接受持久化过程带来的一些开销，那么你可以选择 AOF 持久化，并且根据需要调整同步策略。
- `兼容性和可维护性`：如果你需要在不同版本的 Redis 之间迁移数据或者修复损坏的文件，那么你应该选择 AOF 持久化，因为它使用了通用的 Redis 协议格式。如果你不需要这些功能，那么你可以选择 RDB 持久化。

## 9.怎么实现 Redis 的高可用？(Redis 主从、哨兵、集群)

### 9.1.Redis 主从模式

- `模式简介`：这种模式是通过配置主节点和从节点，实现数据的复制和同步。主节点可以进行读写操作，从节点只能进行读操作。当主节点发生故障时，需要手动或者通过脚本将一个从节点提升为主节点，并让其他从节点指向新的主节点。这种方式可以提高数据的可靠性和读性能，但是不能自动切换主节点，也不能解决写性能和存储容量的问题。

- `实现方式`：在从节点的配置文件 redis.conf 中指定主节点的信息，例如 `slaveof 192.168.1.10 6379` 或者 `replicaof 192.168.1.10 6379`。也可以在启动从节点时，通过参数 `--slaveof` 指定主节点，例如 `./redis-server --slaveof 192.168.1.10 6379`。也可以在运行时，通过命令 `slaveof` 指定主节点，例如 `redis-cli slaveof 192.168.1.10 6379`。

- `实现原理`：
  - 从节点向主节点发送 SYNC 命令
  - 主节点接收到 SYNC 命令后，执行 BGSAVE 命令生成 RDB 文件，并在缓冲区记录后续的写命令
  - 主节点向所有从节点发送快照文件，并在发送期间继续记录写命令
  - 从节点收到快照文件后，丢弃旧数据，载入收到的快照
  - 主节点完成快照发送后，开始向从节点发送缓冲区中的写命令
  - 从节点完成快照载入，开始接受命令请求，并执行来自主节点缓冲区的写命令
  - 主节点每执行一个写命令，都向从节点发送相同的写命令
  - 从节点接收并执行收到的写命令
  - 这样就完成了一次全量同步
  - 之后，主节点执行的每个写命令都会向从节点发送相同的写命令
  - 从节点接收并执行收到的写命令
  - 这样就实现了增量同步

### 9.2.Redis哨兵模式

- `模式简介`：这种模式是在主从的基础上，增加了哨兵节点，用于监控主节点和从节点的状态，并在主节点故障时自动进行故障转移。哨兵节点之间也会相互通信，通过投票机制选举出领导者，负责执行故障转移的操作。这种方式可以实现高可用的自动切换，但是不能解决写性能和存储容量的问题。

- `实现方式`：在每个哨兵节点上运行 redis-sentinel 命令，并配置 sentinel.conf 文件。在 sentinel.conf 文件中指定要监控的主节点和从节点的信息，以及故障转移的策略和参数。例如 `sentinel monitor mymaster 192.168.1.10 6379 2` 表示监控名为 mymaster 的主节点，其 IP 和端口为 192.168.1.10:6379，并且至少需要两个哨兵同意才能执行故障转移。

- `实现原理`：
  - 每个哨兵定期向所有监控的主节点和从节点发送 INFO 命令获取状态信息，并记录在内部数据结构中
  - 每个哨兵定期向其他哨兵发送 HELLO 消息，交换彼此监控到的信息并更新数据结构
  - 当一个哨兵发现某个主节点无法正常响应时，将该主节点标记为主观下线，并向其他哨兵询问该主节点状态
  - 如果一定数量的哨兵也认为该主节点已下线，那么将该主节点标记为客观下线，并开始执行故障转移流程
  - 哨兵向其他哨兵发送请求，要求成为领导者
  - 如果在一定时间内没有收到其他哨兵的反对意见，该哨兵成为领导者，并开始执行故障转移操作
  - 选出一个合适的从节点作为新的主节点
  - 向选定的从节点发送 SLAVEOF NO ONE 命令，让其成为新的主节点
  - 向其他从节点发送 SLAVEOF 命令，让其成为新主节点的从节点
  - 向所有客户端和哨兵广播新主节点的信息
  - 完成故障转移过程

### 9.3.Redis集群模式

- `模式简介`：这种模式是通过分片的方式，将数据分布在多个主节点上，每个主节点都有一个或多个从节点作为备份。当一个主节点发生故障时，集群会自动将其对应的从节点提升为主节点，并继续提供服务。这种方式可以实现高可用的自动切换，同时也可以提高写性能和存储容量。
- `实现方式`：在每个 Redis 节点上运行 redis-server 命令，并配置 redis.conf 文件。在 redis.conf 文件中指定 cluster-enabled 为 yes，并指定 cluster-config-file 和 cluster-node-timeout 等参数。然后使用 redis-cli --cluster create 命令创建集群，并指定各个主节点和从节点的 IP 和端口，以及分片的数量和分配方式。例如 `redis-cli --cluster create 192.168.1.10:6379 192.168.1.11:6379 192.168.1.12:6379 192.168.1.13:6379 192.168.1.14:6379 192.168.1.15:6379 --cluster-replicas 1` 表示创建一个包含三个主节点和三个从节点的集群，每个主节点有一个从节点作为备份，数据分成三个分片。
- `实现原理`：
  - Redis集群将所有数据槽（slot）分成16384个
  - 每个键根据CRC16算法计算出一个值，并对16384取模，确定键属于哪个槽
  - 每个槽对应一个主节点，负责处理该槽的键的读写请求
  - 每个从节点关联一个主节点，作为备份
  - 当主节点发生故障时，集群会使用Raft算法选举出一个领导者
  - 领导者从所有与故障主节点关联的从节点中选出一个合适的从节点
  - 领导者向选定的从节点发送CLUSTER FAILOVER命令，让其成为新的主节点
  - 领导者更新自己的数据结构，记录新旧主节点之间的映射关系
  - 领导者向其他正常的主节点发送消息，让它们也更新自己的数据结构
  - 领导者向所有客户端和从节点广播新主节点的信息
  - 完成故障转移过程

## 10.使用过 Redis 分布式锁嘛？有哪些注意点呢？

> 分布式锁是一种在分布式系统中实现多个客户端对共享资源的互斥访问的机制。分布式锁可以保证数据的一致性和并发控制。Redis分布式锁是一种利用Redis的命令或者Lua脚本来实现分布式锁的方法。

Redis分布式锁有以下几种实现方法：

- **SETNX**：使用SETNX命令来设置一个带有过期时间的键，如果键不存在则返回1，表示加锁成功，否则返回0，表示加锁失败。客户端需要定时检查锁是否过期，如果过期则重新设置。
- **Redlock**：使用多个Redis实例来提高可用性和容错性，客户端需要向至少一半以上的Redis实例请求加锁，并且保证加锁的时间戳一致。如果成功，则认为加锁成功，否则释放所有已经加锁的实例，并重试。
- **SET**：使用SET命令的EX和NX选项来设置一个带有过期时间和唯一标识的键，如果键不存在则返回OK，表示加锁成功，否则返回nil，表示加锁失败。客户端需要使用Lua脚本来释放自己的锁，避免误解别人的锁。
- **Lua脚本**：使用Lua脚本来原子性地执行加锁和解锁的逻辑，避免网络延迟或者客户端崩溃导致的问题。Lua脚本可以直接在Redis服务器上执行，不需要多次通信。

使用Redis分布式锁时，有以下几个注意点：

- 要保证锁的互斥性，即同一时刻只有一个客户端能持有锁。
- 要保证锁的可靠性，即即使客户端崩溃或者网络故障，锁也能被正确释放或者自动过期。
- 要保证锁的正确性，即只有加锁和解锁的是同一个客户端，不能误解别人的锁。
- 要考虑锁的性能和可用性，即尽量减少锁的竞争和等待时间，避免Redis单点故障。

## 11.使用过Redisson嘛？说说它的原理

![image-20230731212641435](/images/ms/image-20230731212641435.png)

> Redisson是一个基于Redis的分布式锁和数据结构的Java框架，它提供了一系列实现了JDK接口的分布式对象和服务，例如RLock, RMap, RList, RSet等。Redisson使用Lua脚本和Redis的命令来实现分布式锁的加锁、解锁、续期、重入等功能，同时利用Redis的发布订阅机制来实现锁的互斥和通知。

下面我简要介绍一下Redisson分布式锁的原理：

- **加锁**：Redisson使用SET命令或者Lua脚本来设置一个带有过期时间和唯一标识（客户端ID+线程ID）的键作为锁，如果键不存在则返回OK，表示加锁成功，否则返回nil，表示加锁失败。如果加锁成功，Redisson会启动一个看门狗线程，定时检查锁是否过期，如果过期则重新设置过期时间，保证锁不会因为客户端崩溃而无法释放。
- **解锁**：Redisson使用Lua脚本来删除锁键，并且校验唯一标识，保证只有加锁的客户端才能解锁。如果解锁成功，Redisson会停止看门狗线程，并且发布一个解锁消息到Redis的频道，通知其他等待的客户端。
- **重入**：Redisson使用一个计数器来记录同一个客户端多次加锁的次数，每次加锁时计数器加一，每次解锁时计数器减一，当计数器为零时才真正释放锁。这样可以保证同一个客户端可以重复获取同一个锁而不会被阻塞。
- **互斥**：Redisson使用一个订阅服务来监听Redis的频道，当有其他客户端释放锁时，会收到一个解锁消息，然后尝试重新获取锁。这样可以避免无效的轮询和等待，提高性能和可用性。

如果您想了解更多关于Redisson分布式锁的原理和实现细节，请参考以下网页：

- [Redisson 实现分布式锁原理分析](https://zhuanlan.zhihu.com/p/135864820)
- [值得收藏，一文掌握 Redisson 分布式锁原理！](https://zhuanlan.zhihu.com/p/344630862)
- [一文掌握 Redisson 分布式锁原理（值得收藏）](https://blog.csdn.net/qq_42046105/article/details/111350721)
- [Redisson原理](https://xuzhijvn.github.io/zh-cn/posts/code/redis/redisson原理/)

## 12.什么是Redlock算法

> RedLock是一种算法,Redlock也就是Redis Distributed Lock,可用多节点redis的分布式锁.
>
> RedLock是官方推荐,Redisson完成了对Redlock算法封装.

此方法具有以下特性：

 (1)互斥访问: 既永远只有一个 client 能拿到锁

 (2)避免死锁: 最终 client 都可能拿到锁,不会出现死锁的情况,即使锁定资源的服务崩溃或者分区,仍然能释放锁

 (3)容错性: 只要大部分 Redis 节点存活 (一半以上) ,就可以正常提供服务

RedLock原理(了解)

1. 获取当前 Unix 时间, 以毫秒为单位.
2. 依次尝试N个实例, 使用相同的 key 和随机值获取锁。在步骤2，当向 Redis 设置锁时，客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端 Redis 已经挂掉的情况，客户端还在死死的等待响应结果。如果服务端没有规定时间内响应，客户端应该尽快尝试另外一个 Redis 实例。
3. 客户端使用当前时间减去开始获取锁时间 （步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（这里是3个节点）的 Redis 节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。
4. 如果取到了锁，key的真正有效时间等于有效时间减去获取锁使用的使时间（步骤3计算的结果）
5. 如果因为某些原因，获取锁失败（没有在至少N/2+1个 Redis 实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即使某些Redis实例根本就没有加锁成功）。

算法示意图如下：

![image-20210829131128229](/images/ms/1630421002-rvFuny-image-20210829131128229.png)

## 13.Redis的跳跃表

Redis跳跃表（跳跃表，Skip List）是一种有序数据结构，用于在Redis中实现有序集合（Sorted Set）。它通过同时维护多层链表来实现高效的查找和插入操作，具有良好的平均时间复杂度。

跳跃表的特点如下：

1. **有序性**：跳跃表中的元素按照从小到大的顺序排列。
2. **多层结构**：跳跃表由多层链表组成，每一层都是当前层次下的元素子集。底层链表包含所有元素，随着层数的增加，链表中的元素逐渐减少。
3. **高效的查找**：由于跳跃表的有序性和多层结构，我们可以通过随机访问每一层链表来进行更快的查找，而不需要像传统链表那样遍历整个链表。
4. **动态性**：跳跃表支持动态地插入、删除和更新元素。

在Redis中，跳跃表主要用于实现有序集合（Sorted Set）数据类型。有序集合中的每个成员都关联一个分数（score），并且根据分数进行排序。跳跃表可以快速地进行基于分数的范围查询和插入/删除操作，使得有序集合在Redis中具有高效的性能。

## 14.MySQL 与Redis 如何保证双写一致性

> MySQL和Redis是两个不同的数据存储系统，它们有不同的特性和用途，因此在实现双写一致性时需要采取一些策略和机制来确保数据的一致性。

以下是一种常见的方式，用于在MySQL和Redis之间实现双写一致性：

1. **通过应用程序层面实现**：应用程序写入数据时，首先将数据写入MySQL，然后再将数据写入到Redis。这样可以确保MySQL和Redis中的数据始终保持一致。如果写入Redis失败或发生错误，可以回滚MySQL的操作，确保数据库的一致性。
2. **使用消息队列**：将写入MySQL和Redis的操作封装成一个消息，并发送到消息队列中。消费者从消息队列中读取消息，并按照顺序执行写入MySQL和Redis的操作。这样可以保证MySQL和Redis的写入顺序一致，从而达到双写一致性。
3. **异步双写**：在写入MySQL后，异步地将数据写入Redis。虽然无法保证实时的双写一致性，但可以通过合理的设计和机制来降低数据不一致性的概率。例如，可以使用定时任务去扫描MySQL中的变更，然后将变更应用到Redis中。

无论采用哪种方式，都需要注意以下几点来确保双写一致性：

- **异常处理**：在写入MySQL或Redis时需要处理异常情况，并进行回滚操作，确保数据的一致性。
- **重试机制**：如果写入Redis失败，可以进行重试操作，直到写入成功或达到最大重试次数为止。
- **监控和报警**：对于数据不一致的情况，需要及时发现并进行处理。建议设置监控和报警机制，用于检测数据一致性的问题。
- **容错处理**：即使在异常情况下，也要确保数据的完整性。可以考虑使用事务、回滚日志等机制来实现数据的容错处理。

## 15.为什么Redis6.0之后改多线程呢

Redis 6.0之前是单线程的，这意味着Redis在处理客户端请求和执行命令时只使用一个线程。这种单线程的设计在Redis的早期版本中是非常有效的，因为它可以避免多线程之间的竞争条件和锁的开销，从而提供了出色的性能。

然而，随着Redis的使用场景不断增加，例如高并发的写入负载、大规模键值对的存储等，单线程逐渐暴露出一些性能瓶颈。为了进一步提升Redis的性能和扩展性，Redis引入了多线程支持。

Redis 6.0之后的多线程改进主要有以下几个原因：

1. **利用多核CPU资源**：多线程使得Redis能够同时利用多个CPU核心，从而实现更好的并发处理能力和更高的吞吐量。这对于需要处理大量请求的应用场景非常重要。
2. **I/O任务的并发处理**：多线程能够帮助Redis更好地处理I/O密集型任务，如读写操作和持久化等。通过将I/O任务分发给不同的线程，可以减少阻塞时间，提高响应速度。
3. **阻塞命令的优化**：在Redis 6.0中，引入了针对阻塞命令（如BRPOP、BLPOP）的专用线程池，以避免阻塞命令对其他客户端请求的影响。这提高了Redis在处理阻塞命令时的并发性能和可靠性。

需要注意的是，Redis 6.0之后的多线程并不意味着完全的并行化执行，某些操作仍然会受到单线程模型的限制。例如，对同一键进行的读写操作仍然需要通过内部的锁机制来保证数据一致性。

总的来说，Redis 6.0之后引入多线程支持是为了更好地适应复杂的应用场景，并提供更高的并发性能和扩展性。但需要根据具体的使用情况来评估是否需要使用多线程模式，以及是否能够发挥多线程带来的性能优势。

## 16.聊聊Redis事务机制

**什么是事务？**

事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。

事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。

**Redis事务的概念**

Redis 事务的本质是通过MULTI、EXEC、WATCH等一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。

总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。

搜索公众号 Java面试题精选，回复“面试资料”，送你一份Java面试宝典.pdf

**Redis事务的三个阶段**

1. 事务开始 MULTI
2. 命令入队
3. 事务执行 EXEC

事务执行过程中，如果服务端收到有EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中排

**事务管理（ACID）概述**

- 原子性（Atomicity）原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。

- 一致性（Consistency）事务前后数据的完整性必须保持一致。

- 隔离性（Isolation）多个事务并发执行时，一个事务的执行不应影响其他事务的执行

- 持久性（Durability）持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响

Redis的事务总是具有ACID中的一致性和隔离性，其他特性是不支持的。当服务器运行在*AOF*持久化模式下，并且appendfsync选项的值为always时，事务也具有耐久性。

**Redis事务支持隔离性吗**

Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的。

**Redis事务保证原子性吗，支持回滚吗**

Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。

**Redis事务其他实现**

- 基于Lua脚本，Redis可以保证脚本内的命令一次性、按顺序地执行，
  其同时也不提供事务运行错误的回滚，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完
- 基于中间标记变量，通过另外的标记变量来标识事务是否执行完成，读取数据时先读取该标记变量判断是否事务执行完成。但这样会需要额外写代码实现，比较繁琐

## 17.Redis的Hash冲突怎么办

首先，我们得知道下HashMap的存储方式，HashMap是k-v的数据结构， 底层是一个entry数组的 数据结构， 并且根据key计算出一个hash值，取模entry数组长度-1，得到一个下标位置来存储数据。 所谓hash冲突 ，就是不同数据hash算法计算出来的结果可能是相同的，那么不同的数据就会放在同 一个下标位置。 

通常解决hash冲突的方法有4种。 

1. 开放寻址法，包含线性探测、平方探测等等，就是从发生冲突的那个位置开始，按照一定的次序从 hash表中找到一个空闲的位置，然后把发生冲突的元素存入到这个空闲位置中。ThreadLocal就用到 了线性探测法来解决hash冲突的。 
2. 链式寻址法，这是一种非常常见的方法，简单理解就是把存在hash冲突的key，以单向链表的方式 来存储。 
3. 再hash法，就是当通过某个hash函数计算的key存在冲突时，再用另外一个hash函数对这个key做 hash，一直运算直到不再产生冲突。这种方式会增加计算时间，性能影响较大。 
4. 建立公共溢出区， 就是把hash表分为基本表和溢出表两个部分，凡事存在冲突的元素，一律放入 到溢出表中。 、

HashMap就是通过链式寻址法来解决的，但是链式地址有个问题：当链表过长的时候，会影响查询 性能，所以在HashMap里，当链表长度大于8的时候，会转为红黑树，提升查询性能。但是树在添加数据的时候也会有树的分裂合并，所以在树节点小于6的时候，又会转为链表。

## 18.在生成 RDB 期间，Redis 可以同时处理写请求么？

1. 可以的，Redis 使用操作系统的多进程**写时复制技术 COW(Copy On Write)** 来实现快照持久化，保证数据一致性。

2. Redis 在持久化时会调用 glibc 的函数`fork`产生一个子进程，快照持久化完全交给子进程来处理，父进程继续处理客户端请求。

3. 当主线程执行写指令修改数据的时候，这个数据就会复制一份副本， `bgsave` 子进程读取这个副本数据写到 RDB 文件。

4. 这既保证了快照的完整性，也允许主线程同时对数据进行修改，避免了对正常业务的影响。

![图片](/images/ms/640-20210815001647641.png)

## 19.Redis底层，使用的什么协议?

Redis 使用的是自定义的基于 TCP 的协议。这个协议被称为 Redis 协议（Redis Protocol），也叫 RESP（REdis Serialization Protocol）。

RESP 是一种文本协议，通过简单的字符串命令和响应来进行通信。它使用明确的规则定义了客户端和服务器之间的交互方式。

RESP 协议的特点包括：

1. 简单易懂：RESP 协议使用文本格式传输，命令和响应都是以明确的字符串形式表示，易于理解和调试。
2. 高效性能：RESP 协议是一种轻量级的协议，因此在网络传输中占用较少的带宽。而且 RESP 的解析和生成过程相对简单，可以提高 Redis 的处理速度。
3. 多样性支持：RESP 协议支持多种类型的数据结构，如字符串、整数、浮点数、数组等，可以很好地满足 Redis 提供的各种功能需求。

需要注意的是，RESP 协议是专门为 Redis 设计的，因此只有 Redis 才能够理解和处理该协议。其他应用程序如果要与 Redis 进行通信，也需要按照 RESP 协议的规范来实现相应的解析和生成逻辑。

## 20.布隆过滤器

**基本原理**：

1. 创建布隆过滤器时，需要设定比特数组的大小和哈希函数的个数。
2. 将要添加的元素通过多个哈希函数映射为比特数组中的多个位置，并将这些位置标记为1。
3. 判断元素是否存在时，同样通过哈希函数计算对应的比特数组位置，并检查这些位置的值是否都为1。若有任一位置为0，则元素不存在于集合中；若所有位置都为1，则元素可能存在于集合中。

**优点**：

1. 空间效率高：布隆过滤器只需要用比特数组和少量的哈希函数来表示集合，相比于其他数据结构，它的存储空间要小得多。
2. 查询速度快：布隆过滤器只需进行哈希和位操作，查询速度非常快。
3. 适用于大规模数据集：由于存储空间小且查询速度快，布隆过滤器在处理大规模数据集时具有优势。

**缺点**：

1. 误判率存在：布隆过滤器会有一定的误判率，即判断某个元素存在于集合中，但实际上并不存在。
2. 无法删除元素：布隆过滤器无法删除元素，因为删除元素会影响其他元素的判断结果。

**应用场景**：

1. 缓存击穿：在高并发环境下，布隆过滤器可以用来判断某个数据是否已存在于缓存中，从而避免对后端存储系统的重复请求。
2. 爬虫去重：在网络爬虫中，布隆过滤器可用于过滤已访问的 URL，以避免重复爬取相同的页面。
3. 黑名单过滤：布隆过滤器可以用于过滤黑名单中的 IP 地址或 URL，从而提高系统的安全性和效率。

**Java 实现**：

1. 在 Java 中，可以使用第三方库如 Guava 提供的布隆过滤器实现。
2. Guava 提供了 BloomFilter 类来创建和操作布隆过滤器。
3. 引入 Guava 依赖后，可以使用 BloomFilter.create() 方法来创建布隆过滤器，并使用 put() 方法添加元素，contains() 方法判断元素是否存在。

## 21.Redis存在线程安全问题吗？为什么？

1. Redis本身是一个单线程的数据库，它采用了事件驱动的模型。在单线程环境下，Redis通过一个事件循环来处理客户端的请求和网络IO操作，这个事件循环是基于I/O多路复用技术实现的，比如select、epoll等。

2. 因为Redis是单线程的，所以不存在线程安全问题。在Redis Server上执行的指令是依次执行的，每次只有一个指令在执行，不会存在并发操作的情况。而对于网络IO操作，Redis使用的是非阻塞IO，可以在一个线程中同时处理多个客户端的请求。

3. 需要注意的是，在Redis 6.0及之后的版本中，引入了多线程模型，但这些多线程只用于处理网络IO事件，而不是执行指令的过程。多线程只是为了更好地利用多核CPU，提高网络IO性能，并不涉及到指令的并发执行。

所以，从Redis的设计和实现上来看，它本身是没有线程安全问题的。

## 22.请描述Redis的缓存淘汰策略

> Redis的缓存淘汰策略是指当Redis的内存使用超过配置的最大值时，如何选择和删除一些数据来腾出空间的方法。

Redis提供了8种缓存淘汰策略，分别是：

- **noeviction**：不进行任何淘汰，只返回错误信息。
- **allkeys-lru**：从所有的键中选择最近最少使用的键进行淘汰。
- **volatile-lru**：从设置了过期时间的键中选择最近最少使用的键进行淘汰。
- **allkeys-random**：从所有的键中随机选择一个键进行淘汰。
- **volatile-random**：从设置了过期时间的键中随机选择一个键进行淘汰。
- **volatile-ttl**：从设置了过期时间的键中选择剩余时间最短的键进行淘汰。
- **volatile-lfu**：从设置了过期时间的键中选择使用频率最低的键进行淘汰。（Redis 4.0后新增）
- **allkeys-lfu**：从所有的键中选择使用频率最低的键进行淘汰。（Redis 4.0后新增）

## 23.Redis多线程模型怎么理解，那它会有线程安全问题吗？

1. Redis从版本6.0开始引入了多线程模型，该模型主要用于处理网络IO事件，提高网络吞吐量和并发连接数的性能。
2. 在多线程模型下，Redis会创建多个子线程来处理网络IO事件，例如接收客户端连接、读写网络数据等。这些子线程是由Redis主线程监听和管理的，它们并行地处理不同的网络IO事件，并将结果返回给主线程。
3. 需要明确的是，Redis的多线程模型仅限于网络IO操作，并不涉及指令的执行过程。也就是说，在Redis中，依然是由主线程按顺序执行各个客户端的指令，保证了指令执行的原子性，因此不会导致线程安全问题。
4. 多线程模型的引入主要是为了优化Redis在高并发场景下的网络性能，并不改变Redis本身单线程执行指令的特性。所以，从指令执行的角度来看，Redis依然是线程安全的。
5. 需要注意的是，在多线程模型下，如果在用户自定义的Redis命令中使用了共享资源（如全局变量），则需要考虑线程安全性和同步机制，以避免出现竞态条件或数据不一致的问题。但是这种情况属于用户自己的代码实现中，与Redis本身的多线程模型无关。

## 24.Redis主从复制的原理

主从架构的核心原理如下：

1. 当启动一个slave节点时，它会向master节点发送PSYNC命令。
2. 如果是slave节点重新连接到master节点，master节点只会复制给slave缺少的那部分数据。
3. 如果是slave节点第一次连接到master节点，将触发完整的数据重新同步。
4. 在完整数据重新同步开始时，master节点会启动后台线程生成RDB快照文件，并将从客户端收到的所有写命令缓存在内存中。生成的RDB文件将发送给slave节点，slave节点先写入本地磁盘，然后加载到内存中。然后，master节点将内存中缓存的写命令发送给slave节点，slave节点同步这些数据。
5. 如果slave节点与master节点之间存在网络故障导致连接断开，slave节点会自动重新连接。master节点如果发现多个slave节点重新连接，只会执行一次RDB保存操作，用一份数据为所有slave节点提供服务。

主从复制的断点续传：

从Redis 2.8版本开始，支持主从复制的断点续传功能。当复制过程中网络连接断开，可以从上次复制的位置继续复制，而不是重新开始。

1. master和slave节点都会保存一个复制偏移量(replica offset)和一个master ID，存储在内存中的backlog中。
2. 如果master和slave之间的网络连接断开，slave节点会要求master节点从上次的复制偏移量开始继续复制。
3. 如果找不到对应的复制偏移量，会执行一次完整的重新同步操作。

无磁盘化复制：

在主节点内存中直接创建RDB文件，并发送给slave节点，不需要在主节点本地落地磁盘。

过期key处理：

slave节点不会单独处理过期key，它只会等待master节点处理过期key。如果master节点过期一个key或通过LRU淘汰一个key，会模拟一条del命令发送给slave节点。

## 25.Redis中AOF重写的过程描述一下

Redis中AOF的重写过程主要包括以下几个步骤：

1. **后台任务触发**：AOF重写可以由两种方式触发。一种是当AOF文件体积超过设定的阈值并且至少存在一个客户端连接时，自动触发重写。另一种方式是通过执行命令`BGREWRITEAOF`手动触发重写。
2. **子进程创建**：当AOF重写被触发时，Redis会创建一个子进程来执行重写操作。
3. **内存缓冲**：为了避免对原AOF文件的读取和写入之间出现不一致的情况，Redis在启动AOF重写时会使用一个内存缓冲区来暂存新的写入命令。
4. **遍历数据库**：重写子进程会从当前的内存数据库中遍历所有的键值对。
5. **生成新AOF文件**：对于每个键值对，重写子进程会将其转换成一条更紧凑和高效的格式，并将其写入到一个临时文件中，即生成一个新的AOF文件。
6. **文件切换**：当重写完成后，Redis会关闭旧的AOF文件，并将新生成的AOF文件重命名为活跃的AOF文件。
7. **继续持久化**：在完成AOF重写期间，Redis会继续将新的写入命令追加到旧的AOF文件和内存缓冲区中，以保持数据的持久化。
8. **重启恢复**：当Redis重新启动时，它会首先加载新的AOF文件进行数据恢复，如果新的AOF文件不存在，则会加载旧的AOF文件。

通过AOF的重写，可以压缩AOF文件的体积，减少磁盘空间占用，并且新的AOF文件可以更快地进行恢复操作。同时，重写过程对Redis的性能影响较小，因为重写操作在后台进行，并且不会阻塞其他客户端请求的处理。

## 26.Redis哨兵机制和集群有什么区别？

1. **功能目标不同：** Redis哨兵机制旨在监控和管理Redis主节点的状态，并在主节点故障时自动进行故障转移。它的目标是保证Redis实例的高可用性。而Redis集群旨在将数据分布在多个节点上，实现数据的自动分片和负载均衡，以提供更高的扩展性和性能。
2. **拓扑结构不同：** Redis哨兵机制使用一组独立运行的哨兵进程，它们监控主节点和从节点的状态。当主节点故障时，哨兵会选举新的主节点并重新配置客户端以使用新的主节点。而Redis集群采用分布式结构，由多个节点组成，每个节点既可以是主节点，也可以是从节点，且节点之间通过集群总线进行通信和协调。
3. **故障处理方式不同：** Redis哨兵机制通过选举新的主节点来进行故障转移。当主节点下线后，哨兵会选举一个从节点作为新的主节点，并通知其他节点更新配置。而Redis集群通过将槽分配给其他节点来处理故障。当一个节点下线时，集群会将该节点的槽分配给其他正常节点，以保证数据的可用性。
4. **数据分布方式不同：** 在Redis哨兵机制中，每个Redis实例独立存储一部分数据，并有一个主节点和零个或多个从节点。而在Redis集群中，数据被自动分片到多个节点上，每个节点负责其中一部分数据。

选择使用Redis哨兵机制还是Redis集群取决于具体的需求。哨兵适用于小规模的高可用性场景，而集群则适用于大规模的分布式环境，需要更高的扩展性和性能。

## 27.谈谈你对Redis的理解

1. Redis是一个基于Key-Value存储结构的NoSQL开源内存数据库。
2. 提供了五种常用的数据类型：String、Map、Set、ZSet、List。
3. 可根据不同的数据结构解决各种场景的问题，如top10问题、好友关注列表、热点话题等。
4. 基于内存存储，并且在数据结构上进行了大量优化，因此具有出色的IO性能。
5. 通常被用作应用程序与数据库之间的分布式缓存组件。
6. 作为非关系型数据存储，无需处理表之间的关联查询，从而提高数据IO效率。
7. 提供了主从复制+哨兵和集群方式实现高可用性。
8. 在Redis集群中，通过哈希槽实现数据分片，进一步提升性能。