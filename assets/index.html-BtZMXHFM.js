import{a as i,c as e,b as s,o as l}from"./app-ZQgO6-gU.js";const t={};function r(n,a){return l(),e("div",null,a[0]||(a[0]=[s(`<h2 id="_1、kafka-的-message-包括哪些信息" tabindex="-1"><a class="header-anchor" href="#_1、kafka-的-message-包括哪些信息"><span>1、Kafka 的 message 包括哪些信息</span></a></h2><p>Kafka 消息由 <strong>固定长度的 header</strong> 和 <strong>变长的消息体 body</strong> 组成。</p><ul><li><p><strong>Header</strong></p><ul><li>1 字节 <code>magic</code>：文件格式标识。</li><li>4 字节 <code>CRC32</code>：用于校验 body 是否完整。</li><li>若 <code>magic=1</code>，header 中额外包含 1 字节 <code>attributes</code>（保存压缩方式、是否压缩等信息）。</li></ul></li><li><p><strong>Body</strong></p><ul><li>N 字节构成消息体，包含具体的 key/value 数据。</li></ul></li></ul><hr><h2 id="_2、怎么查看-kafka-的-offset" tabindex="-1"><a class="header-anchor" href="#_2、怎么查看-kafka-的-offset"><span>2、怎么查看 Kafka 的 offset</span></a></h2><ul><li><p>Kafka 0.9 及以上版本使用最新 Consumer client。</p></li><li><p>方法：</p><div class="language-java line-numbers-mode" data-highlighter="shiki" data-ext="java" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-java"><span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">consumer</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">seekToEnd</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">partitions</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">);</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> // 定位到末尾</span></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">long</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> offset</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> =</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> consumer</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">position</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">partition</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">);</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> // 获取当前最新 offset</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><hr><h2 id="_3、hadoop-的-shuffle-过程" tabindex="-1"><a class="header-anchor" href="#_3、hadoop-的-shuffle-过程"><span>3、Hadoop 的 shuffle 过程</span></a></h2><h3 id="map-端" tabindex="-1"><a class="header-anchor" href="#map-端"><span>Map 端</span></a></h3><ol><li>Map 处理输入数据生成中间结果，写入内存缓冲区。</li><li>当缓冲区满时启动 <strong>spill</strong> 线程写入磁盘。</li><li><strong>二次排序</strong>：先按 partition，再按 key。</li><li><strong>Combiner</strong>（可选）：本地合并减少写入数据量。</li><li>spill 文件写入本地磁盘。</li><li>多 spill 文件通过 <strong>多路归并</strong>生成一个文件。</li></ol><h3 id="reduce-端" tabindex="-1"><a class="header-anchor" href="#reduce-端"><span>Reduce 端</span></a></h3><ol><li><strong>Copy</strong>：从各 Map 拷贝对应 partition 的数据。</li><li><strong>Sort/Merge</strong>：归并排序数据。</li><li><strong>Reduce</strong>：处理最终输出，写入 HDFS。</li></ol><hr><h2 id="_4、spark-集群运算模式" tabindex="-1"><a class="header-anchor" href="#_4、spark-集群运算模式"><span>4、Spark 集群运算模式</span></a></h2><table><thead><tr><th>模式</th><th>描述</th></tr></thead><tbody><tr><td>local</td><td>单机模式，学习/测试用</td></tr><tr><td>standalone</td><td>Master/Worker 模式，可用 ZooKeeper HA</td></tr><tr><td>on Yarn</td><td>依赖 Yarn 资源管理，Spark 调度计算</td></tr><tr><td>on Mesos</td><td>依赖 Mesos 资源管理，Spark 调度计算</td></tr><tr><td>on Cloud</td><td>如 AWS EC2，可访问 S3</td></tr></tbody></table><hr><h2 id="_5、hdfs-读写数据过程" tabindex="-1"><a class="header-anchor" href="#_5、hdfs-读写数据过程"><span>5、HDFS 读写数据过程</span></a></h2><h3 id="读" tabindex="-1"><a class="header-anchor" href="#读"><span>读</span></a></h3><ol><li>客户端请求 Namenode 获取文件块位置信息。</li><li>选择最近的 DataNode 建立 socket。</li><li>DataNode 读取磁盘数据，以 packet 为单位发送。</li><li>客户端接收 packet 写入本地缓存。</li></ol><h3 id="写" tabindex="-1"><a class="header-anchor" href="#写"><span>写</span></a></h3><ol><li>客户端请求 Namenode 上传文件。</li><li>Namenode 返回可上传信息。</li><li>客户端获取每个 block 的 DataNode 列表。</li><li>建立 pipeline：A -&gt; B -&gt; C。</li><li>客户端将数据以 packet 发送给第一个 DataNode，依次转发。</li><li>一个 block 完成后上传下一个 block。</li></ol><hr><h2 id="_6、rdd-中-reducebykey-与-groupbykey-性能对比" tabindex="-1"><a class="header-anchor" href="#_6、rdd-中-reducebykey-与-groupbykey-性能对比"><span>6、RDD 中 reduceByKey 与 groupByKey 性能对比</span></a></h2><table><thead><tr><th>方法</th><th>说明</th></tr></thead><tbody><tr><td>reduceByKey</td><td>Map 端先局部合并，减少网络传输，适合大数据量 reduce</td></tr><tr><td>groupByKey</td><td>全部数据传输到 Reduce 端，易 OOM，性能低</td></tr></tbody></table><p><strong>建议</strong>：大量 reduce 操作使用 reduceByKey。</p><hr><h2 id="_7、spark-2-0-了解" tabindex="-1"><a class="header-anchor" href="#_7、spark-2-0-了解"><span>7、Spark 2.0 了解</span></a></h2><ul><li>更简单：ANSI SQL 与合理 API</li><li>更快：Spark 编译器优化</li><li>更智能：Structured Streaming 支持流处理</li></ul><hr><h2 id="_8、rdd-分区宽依赖与窄依赖" tabindex="-1"><a class="header-anchor" href="#_8、rdd-分区宽依赖与窄依赖"><span>8、RDD 分区宽依赖与窄依赖</span></a></h2><table><thead><tr><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>窄依赖</td><td>父 RDD 分区只被一个子 RDD 分区使用，例如 map、filter、union</td></tr><tr><td>宽依赖</td><td>父 RDD 分区被多个子 RDD 分区使用，例如 groupByKey、reduceByKey、sortByKey，触发 shuffle</td></tr></tbody></table><hr><h2 id="_9、spark-streaming-读取-kafka-数据的两种方式" tabindex="-1"><a class="header-anchor" href="#_9、spark-streaming-读取-kafka-数据的两种方式"><span>9、Spark Streaming 读取 Kafka 数据的两种方式</span></a></h2><ol><li><p><strong>Receiver-based</strong></p><ul><li>使用 Kafka 高层 Consumer API。</li><li>数据存储在 Executor 内存。</li><li>启用 <strong>Write Ahead Log (WAL)</strong> 防止数据丢失。</li></ul></li><li><p><strong>Direct</strong></p><ul><li>Spark 1.3 引入，周期性查询 Kafka offset。</li><li>使用 Kafka 简单 Consumer API 获取指定 offset 范围数据。</li></ul></li></ol><hr><h2 id="_10、kafka-数据存储位置" tabindex="-1"><a class="header-anchor" href="#_10、kafka-数据存储位置"><span>10、Kafka 数据存储位置</span></a></h2><ul><li><p>Kafka 核心思想：顺序读写磁盘。</p></li><li><p>优势：</p><ol><li>Linux 磁盘缓存优化（read-ahead, write-behind）。</li><li>避免 JVM GC 长时间停顿。</li><li>磁盘顺序读写接近内存速度。</li></ol></li></ul><hr><h2 id="_11、kafka-数据丢失解决方法" tabindex="-1"><a class="header-anchor" href="#_11、kafka-数据丢失解决方法"><span>11、Kafka 数据丢失解决方法</span></a></h2><ul><li><strong>Producer</strong>：设置副本数（replication）</li><li><strong>Broker</strong>：多分区均衡分布</li><li><strong>Consumer</strong>：关闭自动提交 offset，消息处理完成后手动提交</li></ul><div class="language-java line-numbers-mode" data-highlighter="shiki" data-ext="java" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-java"><span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">enable</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">auto</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">commit</span><span style="--shiki-light:#999999;--shiki-dark:#666666;"> =</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;"> false</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">;</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">consumer</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">commitSync</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">();</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><hr><h2 id="_12、fsimage-与-edit-区别" tabindex="-1"><a class="header-anchor" href="#_12、fsimage-与-edit-区别"><span>12、fsimage 与 edit 区别</span></a></h2><ul><li><strong>fsimage</strong>：保存最新元数据快照。</li><li><strong>edit</strong>：记录 fsimage 之后的增量修改。</li><li>Checkpoint 时合并生成新的 fsimage。</li></ul><hr><h2 id="_13、hadoop-配置文件优化" tabindex="-1"><a class="header-anchor" href="#_13、hadoop-配置文件优化"><span>13、Hadoop 配置文件优化</span></a></h2><h3 id="core-site-xml" tabindex="-1"><a class="header-anchor" href="#core-site-xml"><span>core-site.xml</span></a></h3><ol><li><code>fs.trash.interval</code>：启用回收站，防止误删。</li><li><code>dfs.namenode.handler.count</code>：增加 Namenode 处理线程数，提高效率。</li><li><code>mapreduce.tasktracker.http.threads</code>：增加 map/reduce 并行传输线程数。</li></ol><hr><h2 id="_14、datanode-首次加入-cluster-不兼容文件版本" tabindex="-1"><a class="header-anchor" href="#_14、datanode-首次加入-cluster-不兼容文件版本"><span>14、DataNode 首次加入 cluster 不兼容文件版本</span></a></h2><ul><li>原因：Namenode 与 DataNode 的 namespaceID、clusterID 不一致。</li><li>处理方法：不格式化 Namenode，手动修改 DataNode 的 namespaceID/clusterID 与 Namenode 保持一致。</li></ul><hr><h2 id="_15、mapreduce-排序阶段及是否可避免" tabindex="-1"><a class="header-anchor" href="#_15、mapreduce-排序阶段及是否可避免"><span>15、MapReduce 排序阶段及是否可避免</span></a></h2><ul><li>Map 阶段：本地输出文件按 key 排序。</li><li>Reduce 阶段：拷贝数据后再排序。</li><li>即使不使用 Combiner，Map 阶段也会排序。</li><li>Hadoop 1.x 不可关闭排序，Hadoop 2.x 可关闭。</li></ul><hr><h2 id="_16、hadoop-优化方法" tabindex="-1"><a class="header-anchor" href="#_16、hadoop-优化方法"><span>16、Hadoop 优化方法</span></a></h2><ul><li>合理设置 block size，减少 namenode 压力。</li><li>使用 Combiner 减少网络传输。</li><li>调整 Mapper/Reducer 数量，提高并行度。</li><li>压缩中间结果（如 map 输出）</li><li>调整 JVM 参数，减少 GC 开销。</li></ul><hr><h2 id="_17、设计题思路" tabindex="-1"><a class="header-anchor" href="#_17、设计题思路"><span>17、设计题思路</span></a></h2><ul><li>明确数据规模，选择合适算法和存储结构。</li><li>考虑分布式处理与内存限制。</li><li>使用 MapReduce/Spark 进行聚合、排序。</li><li>使用哈希、堆或布隆过滤器优化性能。</li></ul><hr><h2 id="_18、top-k-算法处理大文件" tabindex="-1"><a class="header-anchor" href="#_18、top-k-算法处理大文件"><span>18、TOP K 算法处理大文件</span></a></h2><ul><li><p>每个文件读入，统计 query 出现频率。</p></li><li><p>使用 <strong>HashMap + Min-Heap</strong>：</p><ol><li>HashMap 记录 query -&gt; frequency</li><li>Min-Heap 保存 top K</li></ol></li><li><p>文件过大时，分片处理或使用外部排序。</p></li></ul><hr><h2 id="_19、内存不足找出不重复整数" tabindex="-1"><a class="header-anchor" href="#_19、内存不足找出不重复整数"><span>19、内存不足找出不重复整数</span></a></h2><ul><li>使用 <strong>外部排序</strong> 或 <strong>分块哈希</strong>。</li><li>将整数分段，分块处理，每块统计唯一元素，再合并。</li><li>可使用 <strong>位图(BitSet)</strong> 分块处理。</li></ul><hr><h2 id="_20、快速判断一个数是否在-40-亿整数中" tabindex="-1"><a class="header-anchor" href="#_20、快速判断一个数是否在-40-亿整数中"><span>20、快速判断一个数是否在 40 亿整数中</span></a></h2><ul><li>使用 <strong>BitSet</strong>：40 亿位约 500MB，存储状态</li><li>或使用 <strong>Bloom Filter</strong>：内存更小，但可能有误判</li><li>或 <strong>外部排序 + 二分查找</strong>。</li></ul><hr><h2 id="_21、海量数据找重复次数最多元素" tabindex="-1"><a class="header-anchor" href="#_21、海量数据找重复次数最多元素"><span>21、海量数据找重复次数最多元素</span></a></h2><ul><li>MapReduce 或 Spark 聚合统计</li><li>使用 HashMap 或 ReduceByKey 累计次数</li><li>保留最大值或 Top K</li></ul><hr><h2 id="_22、上亿数据统计前-n-个出现次数最多的数据" tabindex="-1"><a class="header-anchor" href="#_22、上亿数据统计前-n-个出现次数最多的数据"><span>22、上亿数据统计前 N 个出现次数最多的数据</span></a></h2><ul><li>使用 <strong>HashMap + Min-Heap</strong>（Top N）</li><li>分块处理，局部 Top N，再合并全局 Top N</li><li>Spark 可用 reduceByKey + takeOrdered(N, -count)</li></ul><hr><h2 id="_23、文本文件统计最频繁出现前-10-个词" tabindex="-1"><a class="header-anchor" href="#_23、文本文件统计最频繁出现前-10-个词"><span>23、文本文件统计最频繁出现前 10 个词</span></a></h2><ul><li>读文件，统计每行词频存入 HashMap</li><li>Min-Heap 保存 top 10</li><li>时间复杂度 O(N log 10) ≈ O(N)</li></ul><hr><h2 id="_24、100w-个数找出最大的-100-个数" tabindex="-1"><a class="header-anchor" href="#_24、100w-个数找出最大的-100-个数"><span>24、100w 个数找出最大的 100 个数</span></a></h2><ul><li>使用 <strong>Min-Heap</strong>，维护 100 个元素</li><li>遍历数据，如果新元素大于堆顶，替换堆顶并调整堆</li><li>时间复杂度 O(N log 100)</li></ul><hr><h2 id="_25、一千万条短信找出重复最多的前-10-条" tabindex="-1"><a class="header-anchor" href="#_25、一千万条短信找出重复最多的前-10-条"><span>25、一千万条短信找出重复最多的前 10 条</span></a></h2><ul><li>类似 Top K 问题</li><li>HashMap 统计出现次数</li><li>Min-Heap 保存 top 10</li><li>若内存不足，可使用 <strong>分块 + 外部排序</strong> 或 Spark 聚合</li></ul>`,84)]))}const d=i(t,[["render",r]]),o=JSON.parse('{"path":"/bigcompany/piaochb4/","title":"唯品会-Java大数据开发","lang":"zh-CN","frontmatter":{"title":"唯品会-Java大数据开发","createTime":"2025/08/21 14:03:59","permalink":"/bigcompany/piaochb4/"},"readingTime":{"minutes":5.1,"words":1530},"git":{"createdTime":1756736713000},"filePathRelative":"notes/bigcompany/company/唯品会-Java大数据开发.md","headers":[]}');export{d as comp,o as data};
